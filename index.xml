<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>CyrusHao</title>
        <link>https://Cyrus-hao.github.io/</link>
        <description>Recent content on CyrusHao</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>zh-cn</language>
        <copyright>CyrusHao</copyright>
        <lastBuildDate>Sat, 10 May 2025 00:00:00 +0000</lastBuildDate><atom:link href="https://Cyrus-hao.github.io/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>MP-SFM</title>
        <link>https://Cyrus-hao.github.io/p/mp-sfm/</link>
        <pubDate>Sat, 10 May 2025 00:00:00 +0000</pubDate>
        
        <guid>https://Cyrus-hao.github.io/p/mp-sfm/</guid>
        <description>&lt;img src="https://Cyrus-hao.github.io/p/mp-sfm/1.png" alt="Featured image of post MP-SFM" /&gt;&lt;h1 id=&#34;510&#34;&gt;5.10
&lt;/h1&gt;&lt;h2 id=&#34;sfm原理学习暂略过数学原理推导&#34;&gt;SFM原理学习(暂略过数学原理推导)
&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;sfm可以通过一组二维图像推算出相机位置/角度以及3D模型&lt;/strong&gt;&lt;/p&gt;
&lt;h3 id=&#34;1-工作原理&#34;&gt;1. 工作原理：
&lt;/h3&gt;&lt;p&gt;通过分析特征点(一般指图像间相同的点)的相对位置关系推算出三维场景，相机位置和角度       &lt;br&gt;
通俗类比下 我拿着相机从不同角度拍一个点 可以通过几何知识 比较不同照片中该点的位置 算出我每次拍照的位置和角度以及该点的三维坐标&lt;/p&gt;
&lt;h3 id=&#34;2-详细步骤&#34;&gt;2. 详细步骤
&lt;/h3&gt;&lt;h4 id=&#34;21-特征提取和匹配找到每张图像中的特征点-并确定哪些点在不同图像中对应同一个现实世界中的点&#34;&gt;2.1 &lt;strong&gt;特征提取和匹配&lt;/strong&gt;:找到每张图像中的特征点 并确定哪些点在不同图像中对应同一个现实世界中的点
&lt;/h4&gt;&lt;p&gt;(1).找到特征点 可以用算法(SIFT ORB或者深度学习的算法 像superpoint)在每张图像中检测一些显著点(像是边缘点 角点等)         &lt;br&gt;
(2).特征描述子(这些点的描述信息)可以用来比较两张图像中的点是否相同    &lt;br&gt;
(3).通过一些匹配算法(SuperGlue或最近邻匹配)可以找到图像中的对应点 类似一个点在两张图像中分别的位置坐标     &lt;br&gt;
&lt;strong&gt;结果&lt;/strong&gt;:可以得到多张图像中的匹配点对 表示它是同一个3D点在不同视角中的投影&lt;/p&gt;
&lt;h4 id=&#34;22-估计相机姿态根据匹配点可以计算每张照片的相机位置和朝向相机外参包括旋转矩阵-r-和平移向量-t&#34;&gt;2.2 &lt;strong&gt;估计相机姿态&lt;/strong&gt;:根据匹配点，可以计算每张照片的相机位置和朝向(相机外参，包括旋转矩阵 R 和平移向量 t)
&lt;/h4&gt;&lt;p&gt;用基础矩阵分析(8点算法或5点算法)匹配点-&amp;gt;推算出两张图像中的相机姿态(一般从两张图像开始计算它们的相对姿态然后逐步加入更多图像-&amp;gt;增量式sfm)   &lt;br&gt;
&lt;strong&gt;结果&lt;/strong&gt;:得到每张图像的相机姿态（在哪里拍,朝哪个方向）&lt;/p&gt;
&lt;h4 id=&#34;23-三维点云重建根据匹配点和相机姿态计算场景中3d点的坐标&#34;&gt;2.3 &lt;strong&gt;三维点云重建&lt;/strong&gt;:根据匹配点和相机姿态，计算场景中3D点的坐标
&lt;/h4&gt;&lt;p&gt;使用三角测量：已知相机位置和匹配点的像素坐标-&amp;gt;计算这些点在3D空间中的位置      &lt;br&gt;
比如，照片A和照片B都看到了一个点，通过它们的像素坐标和相机姿态，可以算出这个点在3D空间的(x, y, z)坐标      &lt;br&gt;
&lt;strong&gt;结果&lt;/strong&gt;：生成一个稀疏点云,表示场景中关键点的3D位置&lt;/p&gt;
&lt;h4 id=&#34;24-全局优化优化相机姿态和3d点的位置减少误差&#34;&gt;2.4 &lt;strong&gt;全局优化&lt;/strong&gt;:优化相机姿态和3D点的位置,减少误差
&lt;/h4&gt;&lt;p&gt;(1).特征匹配和初始计算可能有误差(比如匹配点不完全准确)，需要通过优化算法调整所有参数              &lt;br&gt;
(2).优化目标是最小化重投影误差：即确保3D点投影回每张图像的像素位置与实际观测的像素位置尽可能接近                        &lt;br&gt;
&lt;strong&gt;结果&lt;/strong&gt;:得到更精确的相机姿态和3D点云&lt;/p&gt;
&lt;h4 id=&#34;25-扩展与后处理&#34;&gt;2.5 &lt;strong&gt;扩展与后处理&lt;/strong&gt;
&lt;/h4&gt;&lt;p&gt;(1).扩展：将更多图像加入，重复上述步骤，构建更完整的点云                     &lt;br&gt;
(2).后处理：         &lt;br&gt;
稠密重建：基于稀疏点云，进一步生成稠密点云或网格（比如用 MVS，Multi-View Stereo）       &lt;br&gt;
纹理贴图：将图像的颜色信息映射到 3D 模型上，生成带纹理的模型         &lt;br&gt;
&lt;strong&gt;结果&lt;/strong&gt;:得到完整的 3D 模型，可能包括点云、网格或带纹理的表面&lt;/p&gt;
&lt;h1 id=&#34;511&#34;&gt;5.11
&lt;/h1&gt;&lt;p&gt;跑demo有点问题 pycolmap一直导入不了哇 看到issues上也反映了类似的问题 截止到5.11晚前尚未解决&lt;/p&gt;
&lt;h1 id=&#34;512&#34;&gt;5.12
&lt;/h1&gt;&lt;p&gt;读paper的时候对如下几个问题进行思考            &lt;br&gt;
q1:为什么单眼先验能解决sfm存在对称性导致错误关联的问题      &lt;br&gt;
场景中对称性会引起特征点选择的歧义(传统sfm依靠稀疏关键点进行匹配)-&amp;gt;导致错误的相机位姿估计和3D重建      &lt;br&gt;
单眼先验先用神经网络从单张图像预测深度图 即使是对称也可以明确图像左右侧的深度差异 主要可以引导接下来sfm选择更符合真实几何的配置          &lt;br&gt;
这边想起在小红书上看到周老师的那篇murre的工作(了解了下简单的综述) 主要通过SfM点云去引导条件扩散模型进行单眼深度估计 并用2D基础模型的先验来泛化到包含对称元素的场景&lt;/p&gt;
&lt;p&gt;q2:单眼先验的密集和半密集几何约束是什么             &lt;br&gt;
首先单眼先验是深度学习的模型对单张图像的深度估计或者是表面法向量估计  &lt;br&gt;
密集性主要指深度图为图像中的每个像素提供了深度值-&amp;gt;可以形成了一种连续的几何表达 半密集性主要指模型可能只为高置信度区域（如纹理丰富的区域）生成深度或法向量&lt;/p&gt;
&lt;p&gt;q3:单眼先验在束调整中的正则化作用是什么     &lt;br&gt;
&lt;img src=&#34;https://Cyrus-hao.github.io/p/mp-sfm/2.png&#34;
	width=&#34;1180&#34;
	height=&#34;222&#34;
	srcset=&#34;https://Cyrus-hao.github.io/p/mp-sfm/2_hu_42a769d12aaf7db8.png 480w, https://Cyrus-hao.github.io/p/mp-sfm/2_hu_af5db5a19998f935.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;正则化&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;531&#34;
		data-flex-basis=&#34;1275px&#34;
	
&gt;            &lt;br&gt;
如上图所示 一开始有如下思考 正则化项的值是越小越好吗？ 怎么来衡量这个点是否可靠？ wij是超参数吗？   &lt;br&gt;
首先总的优化目标(损失函数)是重投影误差+lamda*深度正则化项   &lt;br&gt;
(1). 值过小可能过拟合 原本存在一定的系统误差   &lt;br&gt;
(2). wij​就是用来衡量这个点是否可靠  &lt;br&gt;
(3). wij不是超参数 是根据每个点的特性动态计算出来的&lt;/p&gt;
&lt;h1 id=&#34;514&#34;&gt;5.14
&lt;/h1&gt;&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://github.com/cvg/mpsfm/tree/main/third_party&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://github.com/cvg/mpsfm/tree/main/third_party&lt;/a&gt;
这个目录下的所有第三方库都要重新单独clone 其中Sky-Segmentation-and-Post-processing.zip和SuperGluePretrainedNetwork.zip只能下载zip后传到对应文件夹
&lt;img src=&#34;https://Cyrus-hao.github.io/p/mp-sfm/8.png&#34;
	width=&#34;900&#34;
	height=&#34;301&#34;
	srcset=&#34;https://Cyrus-hao.github.io/p/mp-sfm/8_hu_f08bf1343f42409a.png 480w, https://Cyrus-hao.github.io/p/mp-sfm/8_hu_f728168532d47449.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;299&#34;
		data-flex-basis=&#34;717px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;修复OmegaConf的set类型错误                       &lt;br&gt;
在加载colmap_options后添加    &lt;br&gt;
&lt;code&gt;python     del colmap_options[&amp;quot;image_names&amp;quot;]     &lt;/code&gt;   &lt;br&gt;
&lt;img src=&#34;https://Cyrus-hao.github.io/p/mp-sfm/3.png&#34;
	width=&#34;1107&#34;
	height=&#34;558&#34;
	srcset=&#34;https://Cyrus-hao.github.io/p/mp-sfm/3_hu_aa8ae32940c8f255.png 480w, https://Cyrus-hao.github.io/p/mp-sfm/3_hu_6299d24764c4eac9.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;198&#34;
		data-flex-basis=&#34;476px&#34;
	
&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt; 1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 6
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 7
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 8
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 9
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;10
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    cd ceres-solver                           
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    mkdir build     
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    cd build               
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    cmake ..
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    make -j             
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    sudo make install   
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    [-d pyceres] || git clone https://github.com/cvg/pyceres.git   
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    cd pyceres       
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    python -m pip install                  
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;安装pycolmap(&lt;strong&gt;必须要branch中的3.12.0&lt;/strong&gt; 其他版本均会出问题 官方源目前更新到3.11.0)    &lt;br&gt;
其他版本会出现下面的报错&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt; 1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 6
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 7
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 8
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 9
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;10
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;11
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;12
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;13
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;14
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;15
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;16
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;Traceback&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;most&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;recent&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;call&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;last&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;                                                                                                                                                                                     
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;File&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;/datassd2/users/ananth/mpsfm/reconstruct.py&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;line&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;29&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;module&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;gt;&lt;/span&gt;                                                                                                                                             
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;mpsfm_rec&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;experiment&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;                                                                                                                                                                                            
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;File&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;/datassd2/users/ananth/mpsfm/mpsfm/test/simple.py&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;line&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;47&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;fm&#34;&gt;__call__&lt;/span&gt;                                                                                                                                       
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;reconstruction_manager&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;**&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;init_info&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;                                                                                                                                                                    
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;File&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;/datassd2/users/ananth/mpsfm/mpsfm/sfm/reconstruction_manager.py&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;line&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;61&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;fm&#34;&gt;__call__&lt;/span&gt;                                                                                                                        
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;mpsfm_rec&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;_&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;incremental_mapper&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;                                                                                                                                                                            
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;File&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;/datassd2/users/ananth/mpsfm/mpsfm/sfm/mapper/base.py&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;line&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;259&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;fm&#34;&gt;__call__&lt;/span&gt;                                                                                                                                  
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;success&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;registration&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;register_and_triangulate_init_pair&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;init_pair&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;                                                                                                                                         
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;File&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;/datassd2/users/ananth/mpsfm/mpsfm/sfm/mapper/registration.py&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;line&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;107&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;register_and_triangulate_init_pair&lt;/span&gt;                                                                                                
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;candidate_points&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;cam_from_world2&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;_init_pair_points_and_pose&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;**&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;kwargs&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;                                                                                                                                      
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;File&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;/datassd2/users/ananth/mpsfm/mpsfm/sfm/mapper/registration.py&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;line&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;248&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;_init_pair_points_and_pose&lt;/span&gt;                                                                                                        
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;E_info&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;relative_pose_estimator&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;kps1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;matches&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[:,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]],&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;kps2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;matches&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[:,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]],&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;camera1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;camera2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;                                                                                                                  
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;File&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;/datassd2/users/ananth/mpsfm/mpsfm/sfm/estimators/relative_pose.py&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;line&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;15&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;fm&#34;&gt;__call__&lt;/span&gt;                                                                                                                      
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;pycolmap&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;essential_matrix_estimation&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;                                                                                                                                                                       
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;ne&#34;&gt;AttributeError&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;module&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;pycolmap&amp;#39;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;has&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;no&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;attribute&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;essential_matrix_estimation&amp;#39;&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Did&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;you&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;mean&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;essential_matrix_from_pose&amp;#39;&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;?&lt;/span&gt;       
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;https://Cyrus-hao.github.io/p/mp-sfm/6.png&#34;
	width=&#34;1452&#34;
	height=&#34;475&#34;
	srcset=&#34;https://Cyrus-hao.github.io/p/mp-sfm/6_hu_8eff2f9ac7de4448.png 480w, https://Cyrus-hao.github.io/p/mp-sfm/6_hu_b99cb393cf727db6.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;305&#34;
		data-flex-basis=&#34;733px&#34;
	
&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt; 1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 6
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 7
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 8
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 9
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;10
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    git clone https://github.com/Zador-Pataki/colmap.git                 
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    cd colmap                   
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    git checkout mpsfm_colmap_merged             
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    mkdir build &amp;amp;&amp;amp; cd build               
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    cmake .. -DCUDA_ENABLED=ON -DCUDA_ARCH=&amp;#34;86&amp;#34; //根据你自己的显卡架构 3090是86                   
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    make -j$(nproc)                   
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    make install              
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    cd ..             
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    python -m pip install -e .               
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    python -c &amp;#34;import pycolmap; print(pycolmap.__version__)&amp;#34;                                  
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;cmake时会报此错误&lt;br&gt;
&lt;img src=&#34;https://Cyrus-hao.github.io/p/mp-sfm/7.png&#34;
	width=&#34;1687&#34;
	height=&#34;465&#34;
	srcset=&#34;https://Cyrus-hao.github.io/p/mp-sfm/7_hu_6f8ee414168b2900.png 480w, https://Cyrus-hao.github.io/p/mp-sfm/7_hu_9e922a68a291b40f.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;362&#34;
		data-flex-basis=&#34;870px&#34;
	
&gt;&lt;br&gt;
需要自己下载https://github.com/PoseLib/PoseLib/archive/8028473d92c9347794a0e3d3541863b5cbb15743.zip 后mkdir -p _deps/poselib-subbuild/poselib-populate-prefix/ src/ 成 路径(/root/autodl-tmp/mpsfm/colmap/build/_deps/poselib-subbuild/poselib-populate-prefix/src/8028473d92c9347794a0e3d3541863b5cbb15743.zip)&lt;/p&gt;
&lt;p&gt;出现此报错      &lt;br&gt;
&lt;img src=&#34;https://Cyrus-hao.github.io/p/mp-sfm/5.png&#34;
	width=&#34;1242&#34;
	height=&#34;223&#34;
	srcset=&#34;https://Cyrus-hao.github.io/p/mp-sfm/5_hu_f5715ec44121e913.png 480w, https://Cyrus-hao.github.io/p/mp-sfm/5_hu_d116ebd1a64897a4.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;556&#34;
		data-flex-basis=&#34;1336px&#34;
	
&gt;          &lt;br&gt;
则可以强制禁用SiftGPU的CUDA&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;6
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    rm -rf *              
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    cmake .. -DCUDA_ENABLED=OFF -DCUDA_SIFTGPU_ENABLED=OFF -DGPU_ENABLED=OFF -DSIFTGPU_CUDA_ENABLED=OFF         
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    make -j$(nproc)                    
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    make install             
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    cd ..                
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    python -m pip install -e .  //colmap路径下          
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;</description>
        </item>
        <item>
        <title>Week11</title>
        <link>https://Cyrus-hao.github.io/p/week11/</link>
        <pubDate>Mon, 05 May 2025 00:00:00 +0000</pubDate>
        
        <guid>https://Cyrus-hao.github.io/p/week11/</guid>
        <description>&lt;img src="https://Cyrus-hao.github.io/p/week11/1.png" alt="Featured image of post Week11" /&gt;&lt;h1 id=&#34;55&#34;&gt;5.5
&lt;/h1&gt;&lt;p&gt;来自5.6的记录~ 昨日完成了深圳杯数模的实验 简历书写 以及game101前两课 作业未写              &lt;br&gt;
&lt;img src=&#34;https://Cyrus-hao.github.io/p/week11/2.png&#34;
	width=&#34;2736&#34;
	height=&#34;1280&#34;
	srcset=&#34;https://Cyrus-hao.github.io/p/week11/2_hu_dff57a9e8f57de08.png 480w, https://Cyrus-hao.github.io/p/week11/2_hu_a98072d45588f567.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;bt2020_to_display_conversion&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;213&#34;
		data-flex-basis=&#34;513px&#34;
	
&gt;       &lt;br&gt;
&lt;img src=&#34;https://Cyrus-hao.github.io/p/week11/3.png&#34;
	width=&#34;1179&#34;
	height=&#34;1651&#34;
	srcset=&#34;https://Cyrus-hao.github.io/p/week11/3_hu_6702629c186bc087.png 480w, https://Cyrus-hao.github.io/p/week11/3_hu_f895cb478afa5147.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;简历&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;71&#34;
		data-flex-basis=&#34;171px&#34;
	
&gt;&lt;/p&gt;
&lt;h1 id=&#34;56&#34;&gt;5.6
&lt;/h1&gt;&lt;p&gt;早上体育课老师不在 爽空出一节课时间&lt;/p&gt;
&lt;h2 id=&#34;lecture-3&#34;&gt;lecture 3
&lt;/h2&gt;&lt;ol&gt;
&lt;li&gt;2D: point (x,y,1)  vector (x,y,0)&lt;/li&gt;
&lt;li&gt;3D: point (x,y,z,1)  vector (x,y,z,0)&lt;/li&gt;
&lt;li&gt;矩阵*逆矩阵 = 单位矩阵&lt;/li&gt;
&lt;li&gt;矩阵放向量左边 相当于左边的矩阵应用到右边的向量&lt;/li&gt;
&lt;li&gt;矩阵变换从右到左 先进行线性变换再平移                 &lt;br&gt;
&lt;strong&gt;2D transformations&lt;/strong&gt;              &lt;br&gt;
&lt;img src=&#34;https://Cyrus-hao.github.io/p/week11/4.png&#34;
	width=&#34;2559&#34;
	height=&#34;1599&#34;
	srcset=&#34;https://Cyrus-hao.github.io/p/week11/4_hu_f884484fa41dd407.png 480w, https://Cyrus-hao.github.io/p/week11/4_hu_ac21ea85d0954132.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;2D transformations&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;160&#34;
		data-flex-basis=&#34;384px&#34;
	
&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h1 id=&#34;57&#34;&gt;5.7
&lt;/h1&gt;&lt;p&gt;games101目前进度在lecture6 状态有些低迷 满天柳絮啊 确实是对鼻炎患者的谋杀 今天初恋生日 哈哈已经托人祝福她了    &lt;br&gt;
英语周六周日再做吧 这两天打完数模 得多放些时间在nerf和3dgs一些前沿论文上了 论文还是读的太少了&lt;/p&gt;
&lt;h2 id=&#34;plenoxels&#34;&gt;Plenoxels
&lt;/h2&gt;&lt;p&gt;bilbil &lt;a class=&#34;link&#34; href=&#34;https://www.bilibili.com/video/BV1qx4y1u7By?spm_id_from=333.788.videopod.sections&amp;amp;vd_source=2f50930509c568406539e7a29e43c090&#34;  title=&#34;【论文讲解】Plenoxels：没有神经的辐射场，把NeRF提速两个数量级(CVPR 2022)&#34;
     target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;【论文讲解】Plenoxels：没有神经的辐射场，把NeRF提速两个数量级(CVPR 2022)&lt;/a&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;本文提出了Plenoxels，一个用于照片级视角合成的系统。Plenoxels将场景表达为一个稀疏的球谐函数(Spherical Harmonics)表示的体素网格      &lt;br&gt;
这种表达可以不使用任何神经网络，采用包含内外参的图像采用梯度方法和正则化进行优化。&lt;/li&gt;
&lt;li&gt;NeRF的关键点并不是神经网络 而是可微体渲染&lt;/li&gt;
&lt;li&gt;Plenoxels的前作基础是PlenOctrees        &lt;br&gt;
&lt;img src=&#34;https://Cyrus-hao.github.io/p/week11/5.png&#34;
	width=&#34;2559&#34;
	height=&#34;1599&#34;
	srcset=&#34;https://Cyrus-hao.github.io/p/week11/5_hu_529b6dfa6b6bb8e1.png 480w, https://Cyrus-hao.github.io/p/week11/5_hu_72575aac802cface.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;PlenOctrees&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;160&#34;
		data-flex-basis=&#34;384px&#34;
	
&gt;&lt;/li&gt;
&lt;li&gt;把NeRF原作中用MLP预测体密度和RGB值改成预测球谐函数的参数向量k 用k构建这个空间位置处的一系列球谐函数 求和获得color  &lt;br&gt;
&lt;img src=&#34;https://Cyrus-hao.github.io/p/week11/6.png&#34;
	width=&#34;2559&#34;
	height=&#34;1599&#34;
	srcset=&#34;https://Cyrus-hao.github.io/p/week11/6_hu_6931a94be8184055.png 480w, https://Cyrus-hao.github.io/p/week11/6_hu_dfe08308f37ff21.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;pipeline&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;160&#34;
		data-flex-basis=&#34;384px&#34;
	
&gt;             &lt;br&gt;
&lt;img src=&#34;https://Cyrus-hao.github.io/p/week11/7.png&#34;
	width=&#34;2394&#34;
	height=&#34;847&#34;
	srcset=&#34;https://Cyrus-hao.github.io/p/week11/7_hu_be2e0486b48582b5.png 480w, https://Cyrus-hao.github.io/p/week11/7_hu_7e62a8781439538c.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;优化&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;282&#34;
		data-flex-basis=&#34;678px&#34;
	
&gt;&lt;/li&gt;
&lt;li&gt;RGB三个通道 27个球谐函数(3**3) + 体密度1 = 28个&lt;/li&gt;
&lt;li&gt;JAXNeRF 用多GPU加速渲染&lt;/li&gt;
&lt;li&gt;plenoxels用了64个一层层嵌套的球面 为了节约内存 每个球面只用了零阶的球谐函数 和NeRF++一样解决无边界和360度的场景&lt;/li&gt;
&lt;li&gt;总结
&lt;img src=&#34;https://Cyrus-hao.github.io/p/week11/8.png&#34;
	width=&#34;2559&#34;
	height=&#34;1446&#34;
	srcset=&#34;https://Cyrus-hao.github.io/p/week11/8_hu_4847b4d8b81132e6.png 480w, https://Cyrus-hao.github.io/p/week11/8_hu_e0180956328dbcea.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;总结&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;176&#34;
		data-flex-basis=&#34;424px&#34;
	
&gt;&lt;/li&gt;
&lt;li&gt;缺陷
&lt;img src=&#34;https://Cyrus-hao.github.io/p/week11/9.png&#34;
	width=&#34;1713&#34;
	height=&#34;733&#34;
	srcset=&#34;https://Cyrus-hao.github.io/p/week11/9_hu_8e2c15b2c7eff6cc.png 480w, https://Cyrus-hao.github.io/p/week11/9_hu_cb6cab12a1cd5034.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;缺陷&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;233&#34;
		data-flex-basis=&#34;560px&#34;
	
&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h1 id=&#34;58&#34;&gt;5.8
&lt;/h1&gt;&lt;p&gt;bilbil &lt;a class=&#34;link&#34; href=&#34;https://www.bilibili.com/video/BV1Kr42187yX/?spm_id_from=333.999.0.0&amp;amp;vd_source=2f50930509c568406539e7a29e43c090&#34;  title=&#34;大规模与自动驾驶场景重建 | 3D Gaussian Splatting 相关工作整理【1】&#34;
     target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;大规模与自动驾驶场景重建 | 3D Gaussian Splatting 相关工作整理【1】&lt;/a&gt;    &lt;br&gt;
&lt;img src=&#34;https://Cyrus-hao.github.io/p/week11/GS%E6%9C%80%E6%96%B0%E8%BF%9B%E5%B1%951_01.png&#34;
	width=&#34;7680&#34;
	height=&#34;4320&#34;
	srcset=&#34;https://Cyrus-hao.github.io/p/week11/GS%E6%9C%80%E6%96%B0%E8%BF%9B%E5%B1%951_01_hu_bd3d437cc8b63e14.png 480w, https://Cyrus-hao.github.io/p/week11/GS%E6%9C%80%E6%96%B0%E8%BF%9B%E5%B1%951_01_hu_bab023911d4db5d5.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;相关工作&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;177&#34;
		data-flex-basis=&#34;426px&#34;
	
&gt;                  &lt;br&gt;
&lt;img src=&#34;https://Cyrus-hao.github.io/p/week11/GS%E6%9C%80%E6%96%B0%E8%BF%9B%E5%B1%951_02.png&#34;
	width=&#34;7680&#34;
	height=&#34;4320&#34;
	srcset=&#34;https://Cyrus-hao.github.io/p/week11/GS%E6%9C%80%E6%96%B0%E8%BF%9B%E5%B1%951_02_hu_853c1148e9a85299.png 480w, https://Cyrus-hao.github.io/p/week11/GS%E6%9C%80%E6%96%B0%E8%BF%9B%E5%B1%951_02_hu_fa1547fa73388ad1.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;相关工作&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;177&#34;
		data-flex-basis=&#34;426px&#34;
	
&gt;                    &lt;br&gt;
&lt;img src=&#34;https://Cyrus-hao.github.io/p/week11/GS%E6%9C%80%E6%96%B0%E8%BF%9B%E5%B1%951_03.png&#34;
	width=&#34;7680&#34;
	height=&#34;4320&#34;
	srcset=&#34;https://Cyrus-hao.github.io/p/week11/GS%E6%9C%80%E6%96%B0%E8%BF%9B%E5%B1%951_03_hu_49b31e39c003394.png 480w, https://Cyrus-hao.github.io/p/week11/GS%E6%9C%80%E6%96%B0%E8%BF%9B%E5%B1%951_03_hu_f1caf7ef87a40e63.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;相关工作&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;177&#34;
		data-flex-basis=&#34;426px&#34;
	
&gt;             &lt;br&gt;
&lt;img src=&#34;https://Cyrus-hao.github.io/p/week11/GS%E6%9C%80%E6%96%B0%E8%BF%9B%E5%B1%951_04.png&#34;
	width=&#34;7680&#34;
	height=&#34;4320&#34;
	srcset=&#34;https://Cyrus-hao.github.io/p/week11/GS%E6%9C%80%E6%96%B0%E8%BF%9B%E5%B1%951_04_hu_9cdae36e1d303aea.png 480w, https://Cyrus-hao.github.io/p/week11/GS%E6%9C%80%E6%96%B0%E8%BF%9B%E5%B1%951_04_hu_3fe697b76947f943.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;相关工作&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;177&#34;
		data-flex-basis=&#34;426px&#34;
	
&gt;               &lt;br&gt;
&lt;img src=&#34;https://Cyrus-hao.github.io/p/week11/GS%E6%9C%80%E6%96%B0%E8%BF%9B%E5%B1%951_05.png&#34;
	width=&#34;7680&#34;
	height=&#34;4320&#34;
	srcset=&#34;https://Cyrus-hao.github.io/p/week11/GS%E6%9C%80%E6%96%B0%E8%BF%9B%E5%B1%951_05_hu_38e41a69960882e4.png 480w, https://Cyrus-hao.github.io/p/week11/GS%E6%9C%80%E6%96%B0%E8%BF%9B%E5%B1%951_05_hu_30fa60a64c141c38.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;相关工作&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;177&#34;
		data-flex-basis=&#34;426px&#34;
	
&gt;                  &lt;br&gt;
&lt;img src=&#34;https://Cyrus-hao.github.io/p/week11/GS%E6%9C%80%E6%96%B0%E8%BF%9B%E5%B1%951_06.png&#34;
	width=&#34;7680&#34;
	height=&#34;4320&#34;
	srcset=&#34;https://Cyrus-hao.github.io/p/week11/GS%E6%9C%80%E6%96%B0%E8%BF%9B%E5%B1%951_06_hu_aa1fc95192a9e753.png 480w, https://Cyrus-hao.github.io/p/week11/GS%E6%9C%80%E6%96%B0%E8%BF%9B%E5%B1%951_06_hu_ea9950aa9e439cf3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;相关工作&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;177&#34;
		data-flex-basis=&#34;426px&#34;
	
&gt;          &lt;br&gt;
&lt;img src=&#34;https://Cyrus-hao.github.io/p/week11/GS%E6%9C%80%E6%96%B0%E8%BF%9B%E5%B1%951_07.png&#34;
	width=&#34;7680&#34;
	height=&#34;4320&#34;
	srcset=&#34;https://Cyrus-hao.github.io/p/week11/GS%E6%9C%80%E6%96%B0%E8%BF%9B%E5%B1%951_07_hu_9e3c96dd3016cff4.png 480w, https://Cyrus-hao.github.io/p/week11/GS%E6%9C%80%E6%96%B0%E8%BF%9B%E5%B1%951_07_hu_99f714cda4cfd7b1.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;相关工作&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;177&#34;
		data-flex-basis=&#34;426px&#34;
	
&gt;                &lt;br&gt;
&lt;img src=&#34;https://Cyrus-hao.github.io/p/week11/GS%E6%9C%80%E6%96%B0%E8%BF%9B%E5%B1%951_08.png&#34;
	width=&#34;7680&#34;
	height=&#34;4320&#34;
	srcset=&#34;https://Cyrus-hao.github.io/p/week11/GS%E6%9C%80%E6%96%B0%E8%BF%9B%E5%B1%951_08_hu_8c2a4c9fdd129b80.png 480w, https://Cyrus-hao.github.io/p/week11/GS%E6%9C%80%E6%96%B0%E8%BF%9B%E5%B1%951_08_hu_ec1e7f1ef525dd74.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;相关工作&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;177&#34;
		data-flex-basis=&#34;426px&#34;
	
&gt;        &lt;br&gt;
&lt;img src=&#34;https://Cyrus-hao.github.io/p/week11/GS%E6%9C%80%E6%96%B0%E8%BF%9B%E5%B1%951_09.png&#34;
	width=&#34;7680&#34;
	height=&#34;4320&#34;
	srcset=&#34;https://Cyrus-hao.github.io/p/week11/GS%E6%9C%80%E6%96%B0%E8%BF%9B%E5%B1%951_09_hu_3b70b8d61a4cb9.png 480w, https://Cyrus-hao.github.io/p/week11/GS%E6%9C%80%E6%96%B0%E8%BF%9B%E5%B1%951_09_hu_ad47192e33895dd2.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;相关工作&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;177&#34;
		data-flex-basis=&#34;426px&#34;
	
&gt;          &lt;br&gt;
&lt;img src=&#34;https://Cyrus-hao.github.io/p/week11/GS%E6%9C%80%E6%96%B0%E8%BF%9B%E5%B1%951_10.png&#34;
	width=&#34;7680&#34;
	height=&#34;4320&#34;
	srcset=&#34;https://Cyrus-hao.github.io/p/week11/GS%E6%9C%80%E6%96%B0%E8%BF%9B%E5%B1%951_10_hu_55a1e4caa04c91fa.png 480w, https://Cyrus-hao.github.io/p/week11/GS%E6%9C%80%E6%96%B0%E8%BF%9B%E5%B1%951_10_hu_b5878da4c2fa65b2.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;相关工作&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;177&#34;
		data-flex-basis=&#34;426px&#34;
	
&gt;         &lt;br&gt;
&lt;img src=&#34;https://Cyrus-hao.github.io/p/week11/GS%E6%9C%80%E6%96%B0%E8%BF%9B%E5%B1%951_11.png&#34;
	width=&#34;7680&#34;
	height=&#34;4320&#34;
	srcset=&#34;https://Cyrus-hao.github.io/p/week11/GS%E6%9C%80%E6%96%B0%E8%BF%9B%E5%B1%951_11_hu_6d270eeaa847b6d1.png 480w, https://Cyrus-hao.github.io/p/week11/GS%E6%9C%80%E6%96%B0%E8%BF%9B%E5%B1%951_11_hu_ee8717cad424ea1c.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;相关工作&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;177&#34;
		data-flex-basis=&#34;426px&#34;
	
&gt;                   &lt;br&gt;
&lt;img src=&#34;https://Cyrus-hao.github.io/p/week11/GS%E6%9C%80%E6%96%B0%E8%BF%9B%E5%B1%951_12.png&#34;
	width=&#34;7680&#34;
	height=&#34;4320&#34;
	srcset=&#34;https://Cyrus-hao.github.io/p/week11/GS%E6%9C%80%E6%96%B0%E8%BF%9B%E5%B1%951_12_hu_6dfd5b8849007567.png 480w, https://Cyrus-hao.github.io/p/week11/GS%E6%9C%80%E6%96%B0%E8%BF%9B%E5%B1%951_12_hu_32f0511736f2856c.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;相关工作&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;177&#34;
		data-flex-basis=&#34;426px&#34;
	
&gt;                   &lt;br&gt;
&lt;img src=&#34;https://Cyrus-hao.github.io/p/week11/GS%E6%9C%80%E6%96%B0%E8%BF%9B%E5%B1%951_13.png&#34;
	width=&#34;7680&#34;
	height=&#34;4320&#34;
	srcset=&#34;https://Cyrus-hao.github.io/p/week11/GS%E6%9C%80%E6%96%B0%E8%BF%9B%E5%B1%951_13_hu_81c96d68e6ac39bf.png 480w, https://Cyrus-hao.github.io/p/week11/GS%E6%9C%80%E6%96%B0%E8%BF%9B%E5%B1%951_13_hu_129deb95808a1e61.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;相关工作&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;177&#34;
		data-flex-basis=&#34;426px&#34;
	
&gt;   &lt;br&gt;
&lt;img src=&#34;https://Cyrus-hao.github.io/p/week11/GS%E6%9C%80%E6%96%B0%E8%BF%9B%E5%B1%951_14.png&#34;
	width=&#34;7680&#34;
	height=&#34;4320&#34;
	srcset=&#34;https://Cyrus-hao.github.io/p/week11/GS%E6%9C%80%E6%96%B0%E8%BF%9B%E5%B1%951_14_hu_d1717c8b18788f64.png 480w, https://Cyrus-hao.github.io/p/week11/GS%E6%9C%80%E6%96%B0%E8%BF%9B%E5%B1%951_14_hu_efbadd69611b5465.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;相关工作&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;177&#34;
		data-flex-basis=&#34;426px&#34;
	
&gt;         &lt;br&gt;
&lt;img src=&#34;https://Cyrus-hao.github.io/p/week11/GS%E6%9C%80%E6%96%B0%E8%BF%9B%E5%B1%951_15.png&#34;
	width=&#34;7680&#34;
	height=&#34;4320&#34;
	srcset=&#34;https://Cyrus-hao.github.io/p/week11/GS%E6%9C%80%E6%96%B0%E8%BF%9B%E5%B1%951_15_hu_616ade75eb3e9464.png 480w, https://Cyrus-hao.github.io/p/week11/GS%E6%9C%80%E6%96%B0%E8%BF%9B%E5%B1%951_15_hu_4441a07f53cbdc71.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;相关工作&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;177&#34;
		data-flex-basis=&#34;426px&#34;
	
&gt;          &lt;br&gt;
&lt;img src=&#34;https://Cyrus-hao.github.io/p/week11/GS%E6%9C%80%E6%96%B0%E8%BF%9B%E5%B1%951_16.png&#34;
	width=&#34;7680&#34;
	height=&#34;4320&#34;
	srcset=&#34;https://Cyrus-hao.github.io/p/week11/GS%E6%9C%80%E6%96%B0%E8%BF%9B%E5%B1%951_16_hu_51fe1ee2989f369f.png 480w, https://Cyrus-hao.github.io/p/week11/GS%E6%9C%80%E6%96%B0%E8%BF%9B%E5%B1%951_16_hu_dff7f91d7ecf5fdf.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;相关工作&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;177&#34;
		data-flex-basis=&#34;426px&#34;
	
&gt;              &lt;br&gt;
&lt;img src=&#34;https://Cyrus-hao.github.io/p/week11/GS%E6%9C%80%E6%96%B0%E8%BF%9B%E5%B1%951_17.png&#34;
	width=&#34;7680&#34;
	height=&#34;4320&#34;
	srcset=&#34;https://Cyrus-hao.github.io/p/week11/GS%E6%9C%80%E6%96%B0%E8%BF%9B%E5%B1%951_17_hu_c34c217c013b896d.png 480w, https://Cyrus-hao.github.io/p/week11/GS%E6%9C%80%E6%96%B0%E8%BF%9B%E5%B1%951_17_hu_81e35bb0dbd69a59.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;相关工作&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;177&#34;
		data-flex-basis=&#34;426px&#34;
	
&gt;         &lt;br&gt;
&lt;img src=&#34;https://Cyrus-hao.github.io/p/week11/GS%E6%9C%80%E6%96%B0%E8%BF%9B%E5%B1%951_18.png&#34;
	width=&#34;7680&#34;
	height=&#34;4320&#34;
	srcset=&#34;https://Cyrus-hao.github.io/p/week11/GS%E6%9C%80%E6%96%B0%E8%BF%9B%E5%B1%951_18_hu_6f777e5d4d586f83.png 480w, https://Cyrus-hao.github.io/p/week11/GS%E6%9C%80%E6%96%B0%E8%BF%9B%E5%B1%951_18_hu_7827dec1bd65de41.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;相关工作&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;177&#34;
		data-flex-basis=&#34;426px&#34;
	
&gt;                     &lt;br&gt;
&lt;img src=&#34;https://Cyrus-hao.github.io/p/week11/GS%E6%9C%80%E6%96%B0%E8%BF%9B%E5%B1%951_19.png&#34;
	width=&#34;7680&#34;
	height=&#34;4320&#34;
	srcset=&#34;https://Cyrus-hao.github.io/p/week11/GS%E6%9C%80%E6%96%B0%E8%BF%9B%E5%B1%951_19_hu_34d2905bdde14982.png 480w, https://Cyrus-hao.github.io/p/week11/GS%E6%9C%80%E6%96%B0%E8%BF%9B%E5%B1%951_19_hu_37d1508ad424acf8.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;相关工作&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;177&#34;
		data-flex-basis=&#34;426px&#34;
	
&gt;               &lt;br&gt;
&lt;img src=&#34;https://Cyrus-hao.github.io/p/week11/GS%E6%9C%80%E6%96%B0%E8%BF%9B%E5%B1%951_20.png&#34;
	width=&#34;7680&#34;
	height=&#34;4320&#34;
	srcset=&#34;https://Cyrus-hao.github.io/p/week11/GS%E6%9C%80%E6%96%B0%E8%BF%9B%E5%B1%951_20_hu_51e44583179280d2.png 480w, https://Cyrus-hao.github.io/p/week11/GS%E6%9C%80%E6%96%B0%E8%BF%9B%E5%B1%951_20_hu_aa8cd0686769d015.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;相关工作&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;177&#34;
		data-flex-basis=&#34;426px&#34;
	
&gt;&lt;/p&gt;
&lt;h1 id=&#34;59&#34;&gt;5.9
&lt;/h1&gt;&lt;p&gt;感冒稍微好了些 今天鼠群有直播欸 希望能有所收获吧 梦的开始了也是&lt;/p&gt;
&lt;h1 id=&#34;510&#34;&gt;5.10
&lt;/h1&gt;&lt;h2 id=&#34;sfm&#34;&gt;SFM
&lt;/h2&gt;&lt;ol&gt;
&lt;li&gt;sfm可以通过一组二维图像推算出相机位置/角度以及3D模型&lt;/li&gt;
&lt;li&gt;工作原理：   &lt;br&gt;
通过分析特征点(一般指图像间相同的点)的相对位置关系推算出三维场景，相机位置和角度     &lt;br&gt;
通俗类比下 我拿着相机从不同角度拍一个点 可以通过几何知识 比较不同照片中该点的位置 算出我每次拍照的位置和角度以及该点的三维坐标&lt;/li&gt;
&lt;li&gt;详细步骤    &lt;br&gt;
&lt;strong&gt;特征提取和匹配&lt;/strong&gt;:找到每张图像中的特征点 并确定哪些点在不同图像中对应同一个现实世界中的点  &lt;br&gt;
3.1 找到特征点 可以用算法(SIFT ORB或者深度学习的算法 像superpoint)在每张图像中检测一些显著点(像是边缘点 角点等)      &lt;br&gt;
3.2 特征描述子(这些点的描述信息)可以用来比较两张图像中的点是否相同   &lt;br&gt;
3.3 通过一些匹配算法(SuperGlue或最近邻匹配)可以找到图像中的对应点 类似一个点在两张图像中分别的位置坐标   &lt;br&gt;
结果:可以得到多张图像中的匹配点对 表示它是同一个3D点在不同视角中的投影         &lt;br&gt;
&lt;strong&gt;估计相机姿态&lt;/strong&gt;:根据匹配点，可以计算每张照片的相机位置和朝向（相机外参，包括旋转矩阵 R 和平移向量 t）&lt;/li&gt;
&lt;/ol&gt;
</description>
        </item>
        <item>
        <title>Week10</title>
        <link>https://Cyrus-hao.github.io/p/week10/</link>
        <pubDate>Sat, 26 Apr 2025 00:00:00 +0000</pubDate>
        
        <guid>https://Cyrus-hao.github.io/p/week10/</guid>
        <description>&lt;img src="https://Cyrus-hao.github.io/p/week10/1.png" alt="Featured image of post Week10" /&gt;&lt;h1 id=&#34;428&#34;&gt;4.28
&lt;/h1&gt;&lt;h2 id=&#34;denoising-diffusion-probabilistic-models&#34;&gt;Denoising Diffusion Probabilistic Models
&lt;/h2&gt;&lt;p&gt;较真系列起手 这周真是有了吧&lt;br&gt;
bilbil &lt;a class=&#34;link&#34; href=&#34;https://www.bilibili.com/video/BV19H4y1G73r/?spm_id_from=333.1387.homepage.video_card.click&amp;amp;vd_source=2f50930509c568406539e7a29e43c090&#34;  title=&#34;bilbil&#34;
     target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;【较真系列】讲人话-Diffusion Model全解(原理+代码+公式)&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://Cyrus-hao.github.io/p/week10/2.png&#34;
	width=&#34;2287&#34;
	height=&#34;1296&#34;
	srcset=&#34;https://Cyrus-hao.github.io/p/week10/2_hu_47a49482e904fb0.png 480w, https://Cyrus-hao.github.io/p/week10/2_hu_7a659adb27bf5c63.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;训练时&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;176&#34;
		data-flex-basis=&#34;423px&#34;
	
&gt;                 &lt;br&gt;
&lt;img src=&#34;https://Cyrus-hao.github.io/p/week10/3.png&#34;
	width=&#34;2295&#34;
	height=&#34;1302&#34;
	srcset=&#34;https://Cyrus-hao.github.io/p/week10/3_hu_dfd7514bc9b58436.png 480w, https://Cyrus-hao.github.io/p/week10/3_hu_c83afd785f3f2aeb.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;推理时&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;176&#34;
		data-flex-basis=&#34;423px&#34;
	
&gt;             &lt;br&gt;
&lt;img src=&#34;https://Cyrus-hao.github.io/p/week10/4.png&#34;
	width=&#34;2292&#34;
	height=&#34;1300&#34;
	srcset=&#34;https://Cyrus-hao.github.io/p/week10/4_hu_b33d3bfd4d5007f1.png 480w, https://Cyrus-hao.github.io/p/week10/4_hu_9ca47b7706a599a0.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;模型结构&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;176&#34;
		data-flex-basis=&#34;423px&#34;
	
&gt;               &lt;br&gt;
&lt;img src=&#34;https://Cyrus-hao.github.io/p/week10/5.png&#34;
	width=&#34;2287&#34;
	height=&#34;1297&#34;
	srcset=&#34;https://Cyrus-hao.github.io/p/week10/5_hu_d96e1c4b4d6c1680.png 480w, https://Cyrus-hao.github.io/p/week10/5_hu_7f514c263c57b230.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;总结&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;176&#34;
		data-flex-basis=&#34;423px&#34;
	
&gt;&lt;/p&gt;
&lt;h2 id=&#34;ddpm-核心知识要点&#34;&gt;DDPM 核心知识要点
&lt;/h2&gt;&lt;p&gt;Denoising Diffusion Probabilistic Models (DDPM) 是一种基于概率生成模型的深度学习方法，通过逐步添加噪声和去噪的过程生成高质量样本，广泛应用于图像生成等领域。以下是 DDPM 的核心知识要点，涵盖其基本原理、模型结构和训练过程。&lt;/p&gt;
&lt;h3 id=&#34;1-基本原理&#34;&gt;1. 基本原理
&lt;/h3&gt;&lt;p&gt;DDPM 基于马尔可夫链，通过前向和后向过程实现数据生成：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;前向过程（扩散）&lt;/strong&gt;：从真实数据 x_0 开始，逐步添加高斯噪声，经过 T 步生成近似高斯噪声 x_T。每一步的噪声由预定义的方差调度控制。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;后向过程（去噪）&lt;/strong&gt;：从噪声 x_T 开始，通过学习逆向分布逐步恢复原始数据 x_0。后向过程由神经网络参数化，预测去噪方向。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;2-模型结构&#34;&gt;2. 模型结构
&lt;/h3&gt;&lt;p&gt;DDPM 通常采用 U-Net 作为去噪网络，输入为带噪声样本 x_t 和时间步 t，输出为预测的噪声或均值：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;U-Net 架构&lt;/strong&gt;：包含下采样和上采样路径，结合残差连接和注意力机制，适合处理图像数据。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;时间嵌入&lt;/strong&gt;：将时间步 t 编码为正弦/余弦嵌入或全连接层，融入网络以区分不同去噪阶段。&lt;/li&gt;
&lt;li&gt;输出目标通常为噪声 ε 或均值 μ，具体取决于优化目标。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;3-训练过程&#34;&gt;3. 训练过程
&lt;/h3&gt;&lt;p&gt;DDPM 的训练目标是优化后向过程，使生成样本接近真实数据分布：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;损失函数&lt;/strong&gt;：使用简化的变分下界，优化预测噪声与真实噪声的均方误差 (MSE)。流程为：
&lt;ul&gt;
&lt;li&gt;随机采样时间步 t，从真实数据 x_0 生成带噪样本 x_t。&lt;/li&gt;
&lt;li&gt;网络预测噪声，计算预测与实际噪声的 MSE。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;采样&lt;/strong&gt;：训练时随机选择时间步 t 和数据样本，确保模型学习所有去噪阶段。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;优化&lt;/strong&gt;：使用 Adam 优化器通过梯度下降更新网络参数。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;4-推理与采样&#34;&gt;4. 推理与采样
&lt;/h3&gt;&lt;p&gt;生成样本时，从纯高斯噪声 x_T 开始，逐步执行后向去噪过程：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;逐层去噪&lt;/strong&gt;：根据学习到的后向分布，从 x_T 到 x_0 迭代 T 步，每步采样生成下一状态。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;调度优化&lt;/strong&gt;：通过调整方差调度（如 cosine 调度）或加速方法（如 DDIM），减少采样步骤，提高效率。&lt;/li&gt;
&lt;li&gt;生成样本质量高，但推理需要多次前向传播，速度较慢。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;5-关键特性&#34;&gt;5. 关键特性
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;生成质量&lt;/strong&gt;：DDPM 生成的样本保真度高，优于许多传统生成模型（如 GAN）。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;训练稳定&lt;/strong&gt;：基于概率框架，避免了 GAN 的对抗训练问题。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;灵活性&lt;/strong&gt;：支持条件生成（如文本到图像）和其他模态（如音频）。&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;429&#34;&gt;4.29
&lt;/h1&gt;&lt;p&gt;不断告诉自己再不做简历项目就没时间了 真的很难改掉放假前不想学习的心态 明明还有一天 加油&lt;/p&gt;
&lt;h2 id=&#34;理论&#34;&gt;&lt;strong&gt;理论&lt;/strong&gt;
&lt;/h2&gt;&lt;p&gt;bilbil &lt;a class=&#34;link&#34; href=&#34;https://www.bilibili.com/video/BV11e411n79b?spm_id_from=333.788.videopod.sections&amp;amp;vd_source=2f50930509c568406539e7a29e43c090&#34;  title=&#34;bilbil&#34;
     target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;3D Gaussian Splatting原理速通&lt;/a&gt;   &lt;br&gt;
&lt;img src=&#34;https://Cyrus-hao.github.io/p/week10/7.png&#34;
	width=&#34;2559&#34;
	height=&#34;1599&#34;
	srcset=&#34;https://Cyrus-hao.github.io/p/week10/7_hu_4f32fa158477a307.png 480w, https://Cyrus-hao.github.io/p/week10/7_hu_b11f9bd43669b465.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;整体框架&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;160&#34;
		data-flex-basis=&#34;384px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;bilbil &lt;a class=&#34;link&#34; href=&#34;https://www.bilibili.com/video/BV1dpLWzUE7K/?spm_id_from=333.1391.0.0&amp;amp;vd_source=2f50930509c568406539e7a29e43c090&#34;  title=&#34;bilbil&#34;
     target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;从NeRF到3DGS&lt;/a&gt;                       &lt;br&gt;
&lt;img src=&#34;https://Cyrus-hao.github.io/p/week10/6.png&#34;
	width=&#34;1981&#34;
	height=&#34;1104&#34;
	srcset=&#34;https://Cyrus-hao.github.io/p/week10/6_hu_a61aff083880d310.png 480w, https://Cyrus-hao.github.io/p/week10/6_hu_a6e89630af9f4edf.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;总结&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;179&#34;
		data-flex-basis=&#34;430px&#34;
	
&gt;&lt;/p&gt;
&lt;h2 id=&#34;实践&#34;&gt;&lt;strong&gt;实践&lt;/strong&gt;
&lt;/h2&gt;&lt;p&gt;bilbil &lt;a class=&#34;link&#34; href=&#34;https://www.bilibili.com/video/BV1TC4y1M7EU/?spm_id_from=333.788.comment.all.click&amp;amp;vd_source=2f50930509c568406539e7a29e43c090&#34;  title=&#34;bilbil&#34;
     target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;从NeRF到3DGS&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&#34;430&#34;&gt;4.30
&lt;/h1&gt;&lt;p&gt;放假前最后一天 蓝桥杯最后也只是堪堪省三啊 鼠鼠再也不打嵌入式组了🥲 &lt;br&gt;
早上完善了yolo多自注意力+剪枝的项目 找到了更简单的3dgs实现全流程视频     &lt;br&gt;
bilbil &lt;a class=&#34;link&#34; href=&#34;https://www.bilibili.com/video/BV1Fe411o7dU/?spm_id_from=333.1007.top_right_bar_window_history.content.click&amp;amp;vd_source=2f50930509c568406539e7a29e43c090&#34;  title=&#34;bilbil&#34;
     target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;3D Gaussian Splatting本地部署&lt;/a&gt;  &lt;br&gt;
&lt;img src=&#34;https://Cyrus-hao.github.io/p/week10/8.png&#34;
	width=&#34;2559&#34;
	height=&#34;1599&#34;
	srcset=&#34;https://Cyrus-hao.github.io/p/week10/8_hu_5e28c62b994fcbd0.png 480w, https://Cyrus-hao.github.io/p/week10/8_hu_2cffbc822a2bcf40.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;splats&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;160&#34;
		data-flex-basis=&#34;384px&#34;
	
&gt;      &lt;br&gt;
&lt;img src=&#34;https://Cyrus-hao.github.io/p/week10/9.png&#34;
	width=&#34;2559&#34;
	height=&#34;1599&#34;
	srcset=&#34;https://Cyrus-hao.github.io/p/week10/9_hu_14dfd6d18da5a740.png 480w, https://Cyrus-hao.github.io/p/week10/9_hu_bebfb1c90e76cf93.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Ellipsoids&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;160&#34;
		data-flex-basis=&#34;384px&#34;
	
&gt;&lt;/p&gt;
&lt;h1 id=&#34;53&#34;&gt;5.3
&lt;/h1&gt;&lt;p&gt;返校第一天哈 感觉没干什么事 心还在大连。。 深圳杯数模堪堪用ai写完了代码  晚上陪同学过生日聊了会 希望无用的内卷能少些吧。。   &lt;br&gt;
&lt;img src=&#34;https://Cyrus-hao.github.io/p/week10/10.png&#34;
	width=&#34;1993&#34;
	height=&#34;1444&#34;
	srcset=&#34;https://Cyrus-hao.github.io/p/week10/10_hu_7dbd0021115cf65f.png 480w, https://Cyrus-hao.github.io/p/week10/10_hu_c515afd2581355d3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;138&#34;
		data-flex-basis=&#34;331px&#34;
	
&gt;&lt;br&gt;
希望明日早起高效些 简历还缺1项目 这两天尽量在数模完成的同时能大部分构思出 下周结束前一定要完成简历le 加油&lt;/p&gt;
&lt;h1 id=&#34;54&#34;&gt;5.4
&lt;/h1&gt;&lt;p&gt;假期倒数啦 找到篇较好的知乎贴DleX 跟着经验贴完成入门工作似乎也是优解&lt;/p&gt;
&lt;h2 id=&#34;1速通3dgs原理&#34;&gt;1.速通3DGS原理
&lt;/h2&gt;&lt;p&gt;Instant-NGP的速度达到Mip-Nerf360的渲染质量&lt;/p&gt;
&lt;h3 id=&#34;11-sfm初始化稀疏点云&#34;&gt;1.1 sfm初始化稀疏点云
&lt;/h3&gt;&lt;p&gt;通过colmap创建初始化点云&lt;/p&gt;
&lt;h3 id=&#34;12-3d高斯椭球集的创建&#34;&gt;1.2 3D高斯椭球集的创建
&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;位置信息&lt;/strong&gt;：点云信息初始化(x,y,z) 高斯椭球中心点(均值)                                            &lt;br&gt;
&lt;strong&gt;形状信息&lt;/strong&gt;：高斯椭球的协方差矩阵 Y方向压缩再绕旋转原点一定角度                                 &lt;br&gt;
&lt;strong&gt;颜色信息&lt;/strong&gt;：点云颜色(r,g,b)&amp;ndash;用球谐函数来表示 使得点云在不同角度呈现不同颜色                              &lt;br&gt;
&lt;strong&gt;不透明度信息&lt;/strong&gt;：点云不透明度                  &lt;br&gt;
&lt;strong&gt;球谐函数&lt;/strong&gt;：与仰角和方位角有关 与半径无关&lt;/p&gt;
&lt;h3 id=&#34;13-计算机图形学投影矩阵&#34;&gt;1.3 计算机图形学投影矩阵
&lt;/h3&gt;&lt;p&gt;&lt;img src=&#34;https://Cyrus-hao.github.io/p/week10/11.png&#34;
	width=&#34;2559&#34;
	height=&#34;1599&#34;
	srcset=&#34;https://Cyrus-hao.github.io/p/week10/11_hu_82eca5e855d696a8.png 480w, https://Cyrus-hao.github.io/p/week10/11_hu_2f0dba81861b974f.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;计算机图形学投影矩阵&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;160&#34;
		data-flex-basis=&#34;384px&#34;
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;14-渲染公式&#34;&gt;1.4 渲染公式
&lt;/h3&gt;&lt;p&gt;&lt;img src=&#34;https://Cyrus-hao.github.io/p/week10/12.png&#34;
	width=&#34;2559&#34;
	height=&#34;1599&#34;
	srcset=&#34;https://Cyrus-hao.github.io/p/week10/12_hu_61e0164e531cdc7b.png 480w, https://Cyrus-hao.github.io/p/week10/12_hu_6c8fca8d2eb05d39.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;渲染公式&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;160&#34;
		data-flex-basis=&#34;384px&#34;
	
&gt;                      &lt;br&gt;
&lt;strong&gt;权重值*不透明度*颜色值&lt;/strong&gt;&lt;/p&gt;
&lt;h3 id=&#34;15-loss定义&#34;&gt;1.5 loss定义
&lt;/h3&gt;&lt;p&gt;&lt;img src=&#34;https://Cyrus-hao.github.io/p/week10/13.png&#34;
	width=&#34;2559&#34;
	height=&#34;1599&#34;
	srcset=&#34;https://Cyrus-hao.github.io/p/week10/13_hu_a1f3252796f44898.png 480w, https://Cyrus-hao.github.io/p/week10/13_hu_2d1378a83d86fe2.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;渲染公式&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;160&#34;
		data-flex-basis=&#34;384px&#34;
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;16-基于梯度自适应改变点云的分布方式&#34;&gt;1.6 基于梯度自适应改变点云的分布方式
&lt;/h3&gt;&lt;p&gt;pruning离相机近的点和不透明度低于设置的阈值的点 &lt;br&gt;
&lt;strong&gt;过度重构和欠采样&lt;/strong&gt;   &lt;br&gt;
方差很小 &amp;ndash; 克隆高斯来适应  &lt;br&gt;
方差很大 &amp;ndash; 分割成两个高斯&lt;/p&gt;
&lt;h2 id=&#34;2论文讲解用点云结合3d高斯构建辐射场成为快速训练实时渲染的新sota&#34;&gt;2.【论文讲解】用点云结合3D高斯构建辐射场，成为快速训练、实时渲染的新SOTA！
&lt;/h2&gt;&lt;p&gt;bilbil &lt;a class=&#34;link&#34; href=&#34;https://www.bilibili.com/video/BV1uV4y1Y7cA/?spm_id_from=333.788.top_right_bar_window_default_collection.content.click&amp;amp;vd_source=2f50930509c568406539e7a29e43c090&#34;  title=&#34;bilbil&#34;
     target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;【论文讲解】用点云结合3D高斯构建辐射场，成为快速训练、实时渲染的新SOTA！&lt;/a&gt;        &lt;br&gt;
&lt;img src=&#34;https://Cyrus-hao.github.io/p/week10/14.png&#34;
	width=&#34;2559&#34;
	height=&#34;1599&#34;
	srcset=&#34;https://Cyrus-hao.github.io/p/week10/14_hu_e5b94d90d580ccb7.png 480w, https://Cyrus-hao.github.io/p/week10/14_hu_16970f44c9dc353.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;160&#34;
		data-flex-basis=&#34;384px&#34;
	
&gt;&lt;br&gt;
&lt;img src=&#34;https://Cyrus-hao.github.io/p/week10/15.png&#34;
	width=&#34;1471&#34;
	height=&#34;414&#34;
	srcset=&#34;https://Cyrus-hao.github.io/p/week10/15_hu_2b610e63f8d3b66a.png 480w, https://Cyrus-hao.github.io/p/week10/15_hu_8fd05c432a3d412b.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;pipline&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;355&#34;
		data-flex-basis=&#34;852px&#34;
	
&gt;              &lt;br&gt;
&lt;strong&gt;总结&lt;/strong&gt;：从初始sfm点云出发 以每个点为中心生成3D高斯 用相机参数把点投影到图像平面上(splatting) 从splatting的痕迹中进行tile-based的光栅化得到渲染图象 将渲染图像和
GT图像求loss反向传播 自适应的密度控制模块根据传递到点上的梯度来决定是否需要对3D高斯做分割或者克隆 梯度也会传递到3D高斯里来更新其中储存的位置 协方差矩阵 不透明度等
参数               &lt;br&gt;
&lt;strong&gt;核心&lt;/strong&gt;:构建以协方差矩阵为主导的3D高斯点云 围绕3D高斯点云进行渲染和优化&lt;/p&gt;
</description>
        </item>
        <item>
        <title>Week9</title>
        <link>https://Cyrus-hao.github.io/p/week9/</link>
        <pubDate>Sat, 26 Apr 2025 00:00:00 +0000</pubDate>
        
        <guid>https://Cyrus-hao.github.io/p/week9/</guid>
        <description>&lt;img src="https://Cyrus-hao.github.io/p/week9/p8.png" alt="Featured image of post Week9" /&gt;&lt;h1 id=&#34;426&#34;&gt;4.26
&lt;/h1&gt;&lt;p&gt;鼠鼠打算试试申暑研  善良的老师推了zju的一位做3DGS的大牛  遂开始换方向开始3DGS方向的学习  昨晚和做定位的高手学长聊了下  收获满满捏  周六较闲  开始该方向论文的检索&lt;/p&gt;
&lt;h2 id=&#34;vggt-visual-geometry-grounded-transformer&#34;&gt;VGGT: Visual Geometry Grounded Transformer
&lt;/h2&gt;&lt;p&gt;&lt;img src=&#34;https://Cyrus-hao.github.io/p/week9/p3.png&#34;
	width=&#34;738&#34;
	height=&#34;228&#34;
	srcset=&#34;https://Cyrus-hao.github.io/p/week9/p3_hu_79d0df0c61db294a.png 480w, https://Cyrus-hao.github.io/p/week9/p3_hu_1aae8fb97ba46fdd.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;previous works&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;323&#34;
		data-flex-basis=&#34;776px&#34;
	
&gt;              &lt;br&gt;
介绍了一种前馈神经网络VGGT，能够从单个或多个图像视图中直接预测场景的3D属性，包括相机参数、点图、深度图和3D点轨迹。该模型在多个3D任务中表现出色，如相机参数估计、多视图深度估计和密集点云重建，且处理速度快（不到一秒）。               &lt;br&gt;
&lt;img src=&#34;https://Cyrus-hao.github.io/p/week9/p1.png&#34;
	width=&#34;1452&#34;
	height=&#34;478&#34;
	srcset=&#34;https://Cyrus-hao.github.io/p/week9/p1_hu_905b4a5ba117532.png 480w, https://Cyrus-hao.github.io/p/week9/p1_hu_74ed7c00cf1c3f97.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Architecture Overview&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;303&#34;
		data-flex-basis=&#34;729px&#34;
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;一-输入图像处理&#34;&gt;(一) 输入图像处理
&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;图像分块与特征提取:&lt;/strong&gt;     &lt;br&gt;
VGGT首先将输入图像分成小块，使用DINOv2(选择DINOv2是因为其在训练早期阶段表现更稳定，且对超参数（如学习率和动量）不敏感，相比之下，14×14的卷积层在初期可能不稳定)提取特征生成tokens，并添加位置嵌入以保留空间信息 这一步确保模型能稳定处理图像。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;位置嵌入:&lt;/strong&gt;                        &lt;br&gt;
在tokens上添加位置嵌入 以保留图像的空间关系。这一步对于保持transformer对空间信息的感知至关重要。&lt;/p&gt;
&lt;h3 id=&#34;二-transformer架构&#34;&gt;(二) Transformer架构
&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;交替注意力机制&lt;/strong&gt;：    &lt;br&gt;
模型采用24层的transformer，使用交替注意力机制，交替处理帧内和全局信息。每个注意力层有1024维特征和16个头，类似于DINOv2的ViT-L配置。&lt;/p&gt;
&lt;p&gt;注意力机制交替在帧内（frame-wise）和全局（global）之间工作：      &lt;br&gt;
&lt;strong&gt;帧内注意力：&lt;/strong&gt; 关注单个图像帧内的局部细节，适合捕捉细粒度的特征。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;全局注意力：&lt;/strong&gt; 跨多个视图理解全局场景，适合处理多视图一致性。&lt;/p&gt;
&lt;h3 id=&#34;三-特征提取与上采样&#34;&gt;(三) 特征提取与上采样
&lt;/h3&gt;&lt;p&gt;从特定层(4、11、17和23)提取tokens，通过DPT进行上采样，生成如深度图的高分辨率输出。这一步支持3D属性的精确预测。&lt;/p&gt;
&lt;h3 id=&#34;四-训练数据增强&#34;&gt;(四) 训练数据增强
&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;图像增强技术：&lt;/strong&gt;                  &lt;br&gt;
在训练过程中，随机应用以下增强，以提高模型对各种光照和噪声条件的鲁棒性：&lt;/p&gt;
&lt;p&gt;颜色抖动（Color Jittering）：调整图像的亮度、对比度、饱和度和色调，模拟不同光照条件。&lt;/p&gt;
&lt;p&gt;高斯模糊（Gaussian Blur）：应用模糊效果，模拟真实世界中的成像模糊。&lt;/p&gt;
&lt;p&gt;灰度增强（Grayscale Augmentation）：将图像转换为灰度，测试模型在无颜色信息时的性能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;图像尺寸调整：&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;图像、深度图和点图被等比例调整，使最大维度为518像素。&lt;/p&gt;
&lt;p&gt;较短的维度被裁剪到168~518像素之间，且必须是14像素的倍数，以匹配patchification的patch大小。这一步确保了输入数据的统一性和兼容性。&lt;/p&gt;
&lt;h3 id=&#34;五-总结&#34;&gt;(五) 总结
&lt;/h3&gt;&lt;p&gt;&lt;img src=&#34;https://Cyrus-hao.github.io/p/week9/p2.png&#34;
	width=&#34;1291&#34;
	height=&#34;448&#34;
	srcset=&#34;https://Cyrus-hao.github.io/p/week9/p2_hu_ae0a6be6c3c1cd48.png 480w, https://Cyrus-hao.github.io/p/week9/p2_hu_31d231c88f3d25ed.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;VGGT图像转换方法总结&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;288&#34;
		data-flex-basis=&#34;691px&#34;
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;pointmap-branch-和-camera-branch&#34;&gt;Pointmap Branch 和 Camera Branch
&lt;/h3&gt;&lt;p&gt;&lt;img src=&#34;https://Cyrus-hao.github.io/p/week9/p4.png&#34;
	width=&#34;1308&#34;
	height=&#34;922&#34;
	srcset=&#34;https://Cyrus-hao.github.io/p/week9/p4_hu_747d2ce27f65867.png 480w, https://Cyrus-hao.github.io/p/week9/p4_hu_e074fa679ab70c15.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Pointmap Branch 和 Camera Branch&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;141&#34;
		data-flex-basis=&#34;340px&#34;
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;camera-branch&#34;&gt;Camera Branch
&lt;/h3&gt;&lt;p&gt;&lt;img src=&#34;https://Cyrus-hao.github.io/p/week9/p5.png&#34;
	width=&#34;2479&#34;
	height=&#34;1485&#34;
	srcset=&#34;https://Cyrus-hao.github.io/p/week9/p5_hu_a9f202e999261a5d.png 480w, https://Cyrus-hao.github.io/p/week9/p5_hu_e9857fc9977a3e30.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Camera Branch&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;166&#34;
		data-flex-basis=&#34;400px&#34;
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;pointmap-branch&#34;&gt;Pointmap Branch
&lt;/h3&gt;&lt;p&gt;&lt;img src=&#34;https://Cyrus-hao.github.io/p/week9/p6.png&#34;
	width=&#34;2479&#34;
	height=&#34;1485&#34;
	srcset=&#34;https://Cyrus-hao.github.io/p/week9/p6_hu_4ee3df471952043a.png 480w, https://Cyrus-hao.github.io/p/week9/p6_hu_43b68b685c11255d.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Pointmap Branch&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;166&#34;
		data-flex-basis=&#34;400px&#34;
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;code&#34;&gt;Code
&lt;/h3&gt;&lt;p&gt;&lt;img src=&#34;https://Cyrus-hao.github.io/p/week9/p9.png&#34;
	width=&#34;661&#34;
	height=&#34;610&#34;
	srcset=&#34;https://Cyrus-hao.github.io/p/week9/p9_hu_30e5f7b0ee10a0fa.png 480w, https://Cyrus-hao.github.io/p/week9/p9_hu_d96d78d27a9f3815.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;代码文件功能对比&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;108&#34;
		data-flex-basis=&#34;260px&#34;
	
&gt;&lt;/p&gt;
&lt;h4 id=&#34;代码文件初步评估&#34;&gt;代码文件初步评估
&lt;/h4&gt;&lt;ol&gt;
&lt;li&gt;demo_gradio.py&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;作用&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;功能：&lt;/strong&gt; 提供一个基于 Gradio 的交互式 Web 界面，允许用户上传图像或视频，进行 3D 点云重建，并通过 3D 查看器可视化结果（以 GLB 格式输出）。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;核心任务：&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;处理用户上传的视频（提取帧）或图像，存储到临时目录。&lt;/li&gt;
&lt;li&gt;使用 VGGT 模型进行推理，生成 3D 点云（Pointmap Branch 或 Depthmap + Camera Branch）和相机参数。&lt;/li&gt;
&lt;li&gt;将预测结果转换为 GLB 文件，显示在 Gradio 的 3D 查看器中。&lt;/li&gt;
&lt;li&gt;支持交互式参数调整（如置信度阈值、天空过滤、帧选择、相机显示）。&lt;/li&gt;
&lt;li&gt;提供示例数据（预定义视频）展示重建效果。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;关键函数：&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;run_model：运行 VGGT 模型推理，生成点云和相机参数。&lt;/li&gt;
&lt;li&gt;handle_uploads：处理上传的视频/图像，创建临时目录。&lt;/li&gt;
&lt;li&gt;gradio_demo：执行重建，生成 GLB 文件。&lt;/li&gt;
&lt;li&gt;update_visualization：根据用户调整的参数更新可视化。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;输出：&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;3D 点云和相机姿态的 GLB 文件。&lt;/li&gt;
&lt;li&gt;交互式界面，包含画廊、3D 查看器和参数控件。                     &lt;br&gt;
**依赖：**依赖 visual_util.py 的 predictions_to_glb 函数来生成 GLB 文件。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;重要性&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;这是用户直接交互的主要入口，提供最直观的 3D 重建体验。&lt;/li&gt;
&lt;li&gt;集成了完整的处理流程（上传、推理、可视化、参数调整），是系统的核心前端。&lt;/li&gt;
&lt;li&gt;适合展示和测试 VGGT 模型的功能，广泛适用于研究、演示和开发。&lt;/li&gt;
&lt;li&gt;示例数据和交互式界面使其对非技术用户友好，具有高可用性。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;demo_viser.py&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;作用&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;功能：&lt;/strong&gt; 使用 Viser（一个 Python 3D 可视化库）提供基于 Web 的 3D 点云和相机姿态可视化，支持实时交互。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;核心任务：&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;从 VGGT 模型的预测结果（点云、相机参数、置信度等）生成 3D 点云和相机视锥（frustum）。&lt;/li&gt;
&lt;li&gt;通过 Viser 创建 Web 服务器，展示点云和相机姿态。&lt;/li&gt;
&lt;li&gt;支持动态调整置信度阈值、帧选择和相机显示。&lt;/li&gt;
&lt;li&gt;可选择使用 Pointmap Branch（world_points）或 Depthmap + Camera Branch（world_points_from_depth）。&lt;/li&gt;
&lt;li&gt;支持天空分割（mask_sky）以过滤天空区域的点。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;关键函数：&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;viser_wrapper：主函数，设置 Viser 服务器，渲染点云和相机视锥。&lt;/li&gt;
&lt;li&gt;apply_sky_segmentation：应用天空分割，过滤天空点。&lt;/li&gt;
&lt;li&gt;main：加载模型，处理图像，运行推理，启动 Viser 可视化。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;输出：&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;通过 Web 界面（默认端口 8080）展示的 3D 点云和相机姿态。&lt;/li&gt;
&lt;li&gt;支持交互式调整（置信度、帧选择、相机显示）。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;依赖：&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;依赖 visual_util.py 的 segment_sky 和 download_file_from_url 函数处理天空分割。&lt;/li&gt;
&lt;li&gt;依赖 VGGT 模型相关的工具函数（load_and_preprocess_images, pose_encoding_to_extri_intri, unproject_depth_map_to_point_map）。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;重要性&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;提供了一种替代 Gradio 的可视化方式，适合需要更高交互性或本地 Web 服务器的场景。&lt;/li&gt;
&lt;li&gt;支持与 Gradio 类似的功能（点云、相机姿态、天空分割），但更专注于 3D 渲染，可能在某些场景下（如研究或调试）提供更好的可视化体验。&lt;/li&gt;
&lt;li&gt;依赖命令行参数运行，适合技术用户或批量处理，但对非技术用户不如 Gradio 友好。&lt;/li&gt;
&lt;li&gt;如果项目主要使用 Gradio 界面，Viser 的作用可能次要。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start=&#34;3&#34;&gt;
&lt;li&gt;vggt_to_colmap.py&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;作用&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;功能：&lt;/strong&gt; 将 VGGT 模型的预测结果（点云、相机参数）转换为 COLMAP 格式的文件（cameras.txt/bin, images.txt/bin, points3D.txt/bin），用于后续 SfM（Structure from Motion）或 MVS（Multi-View Stereo）处理。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;核心任务：&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;加载 VGGT 模型，处理输入图像，生成 3D 点云和相机参数。&lt;/li&gt;
&lt;li&gt;过滤点云（基于置信度、天空、背景颜色），生成 COLMAP 兼容的 3D 点和 2D-3D 对应关系。&lt;/li&gt;
&lt;li&gt;将相机内外参、姿态和点云数据写入 COLMAP 格式（支持文本或二进制文件）。&lt;/li&gt;
&lt;li&gt;支持天空分割和背景过滤（黑色/白色）。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;关键函数：&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;process_images：运行 VGGT 模型推理，生成预测。&lt;/li&gt;
&lt;li&gt;filter_and_prepare_points：过滤点云，生成 COLMAP 格式的 3D 点和 2D 投影。&lt;/li&gt;
&lt;li&gt;write_colmap_*：写入 COLMAP 格式文件（cameras, images, points3D）。&lt;/li&gt;
&lt;li&gt;segment_sky：应用天空分割（与 visual_util.py 共享）。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;输出：&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;COLMAP 格式文件（cameras.txt/bin, images.txt/bin, points3D.txt/bin），可用于 COLMAP 或其他 SfM/MVS 工具。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;依赖：&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;依赖 visual_util.py 的 segment_sky 和 download_file_from_url 函数。&lt;/li&gt;
&lt;li&gt;依赖 VGGT 模型相关工具函数。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;重要性&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;对于需要将 VGGT 输出集成到 COLMAP 工作流（例如进一步优化相机姿态或生成稠密点云）的用户来说至关重要。&lt;/li&gt;
&lt;li&gt;提供了从深度学习预测到传统 SfM/MVS 工具的桥梁，扩展了 VGGT 的应用场景。&lt;/li&gt;
&lt;li&gt;依赖命令行运行，适合技术用户或自动化流程，但不如 Gradio 界面直观。&lt;/li&gt;
&lt;li&gt;如果项目不涉及 COLMAP 或 SfM，其重要性较低。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start=&#34;4&#34;&gt;
&lt;li&gt;visual_util.py&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;作用&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;功能：&lt;/strong&gt; 提供通用的工具函数，支持 3D 点云和相机姿态的处理、可视化以及天空分割。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;核心任务：&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;将 VGGT 预测转换为 GLB 格式的 3D 场景（predictions_to_glb），用于 Gradio 和其他可视化。&lt;/li&gt;
&lt;li&gt;处理相机姿态、点云过滤（置信度、天空、背景颜色）和场景对齐。&lt;/li&gt;
&lt;li&gt;提供天空分割功能（segment_sky, run_skyseg），支持过滤天空区域的点。&lt;/li&gt;
&lt;li&gt;提供文件下载功能（download_file_from_url），用于获取天空分割模型。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;关键函数：&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;predictions_to_glb：将点云和相机参数转换为 GLB 场景，支持过滤和相机可视化。&lt;/li&gt;
&lt;li&gt;integrate_camera_into_scene：将相机模型添加到 3D 场景。&lt;/li&gt;
&lt;li&gt;segment_sky：使用 ONNX 模型进行天空分割。&lt;/li&gt;
&lt;li&gt;download_file_from_url：处理 Hugging Face 模型文件的下载。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;输出：&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;GLB 格式的 3D 场景（用于 Gradio 或其他工具）。&lt;/li&gt;
&lt;li&gt;天空分割掩码（用于点云过滤）。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;依赖：&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;依赖外部库（如 trimesh, onnxruntime）。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;重要性&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;是其他三个文件（demo_gradio.py, demo_viser.py, vggt_to_colmap.py）的核心依赖，提供关键的点云处理和可视化功能。&lt;/li&gt;
&lt;li&gt;predictions_to_glb 是 Gradio 界面生成 3D 可视化的基础。&lt;/li&gt;
&lt;li&gt;天空分割和文件下载功能被多个文件共享，增强了点云过滤的通用性。&lt;/li&gt;
&lt;li&gt;作为一个工具模块，其代码被广泛复用，是系统不可或缺的部分。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;代码文件详细解析&#34;&gt;代码文件详细解析
&lt;/h4&gt;&lt;h4 id=&#34;demo_gradiopy&#34;&gt;demo_gradio.py
&lt;/h4&gt;&lt;h5 id=&#34;1-模型加载与推理run_model-函数&#34;&gt;1. 模型加载与推理（run_model 函数）
&lt;/h5&gt;&lt;p&gt;&lt;strong&gt;功能：&lt;/strong&gt;                     &lt;br&gt;
从指定目录加载图像，运行 VGGT 模型推理，生成 3D 点云和相机参数。                 &lt;br&gt;
支持 Pointmap Branch（直接预测 3D 点）和 Depthmap + Camera Branch（通过深度图和相机参数生成点云）。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;6
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;7
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;images&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;load_and_preprocess_images&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;image_names&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;to&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;device&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; 
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;k&#34;&gt;with&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;torch&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;no_grad&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;():&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;k&#34;&gt;with&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;torch&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;cuda&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;amp&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;autocast&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;dtype&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;dtype&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;            &lt;span class=&#34;n&#34;&gt;predictions&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;model&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;images&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;extrinsic&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;intrinsic&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;pose_encoding_to_extri_intri&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;predictions&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;pose_enc&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;images&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;shape&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:])&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;depth_map&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;predictions&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;depth&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;world_points&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;unproject_depth_map_to_point_map&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;depth_map&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;predictions&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;extrinsic&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;predictions&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;intrinsic&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;])&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;&lt;strong&gt;输入：&lt;/strong&gt;            &lt;br&gt;
图像目录（target_dir/images）中的图像文件。               &lt;br&gt;
&lt;strong&gt;预处理：&lt;/strong&gt;      &lt;br&gt;
通过 load_and_preprocess_images 将图像转换为张量（形状 [S, C, H, W]，S 为图像数量）。                 &lt;br&gt;
&lt;strong&gt;推理：&lt;/strong&gt;            &lt;br&gt;
使用 VGGT 模型生成预测，包括 depth（深度图）、world_points（Pointmap Branch 的 3D 点图）、pose_enc（Camera Branch 的姿态编码）。           &lt;br&gt;
自动混合精度（torch.cuda.amp.autocast）提高推理效率。                            &lt;br&gt;
&lt;strong&gt;后处理：&lt;/strong&gt;            &lt;br&gt;
pose_enc 转换为相机内外参（extrinsic 和 intrinsic）。                  &lt;br&gt;
深度图通过 unproject_depth_map_to_point_map 结合相机参数生成 3D 点（world_points_from_depth）。          &lt;br&gt;
&lt;strong&gt;输出：&lt;/strong&gt;                   &lt;br&gt;
predictions 字典，包含 depth、 world_points、 world_points_from_depth、 extrinsic、 intrinsic 等。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Pointmap Branch：&lt;/strong&gt;          &lt;br&gt;
输出 predictions[&amp;ldquo;world_points&amp;rdquo;]，直接预测每个像素的 3D 坐标（形状 [S, H, W, 3]）。          &lt;br&gt;
在 prediction_mode=&amp;ldquo;Pointmap Regression&amp;rdquo; 时使用。      &lt;br&gt;
&lt;strong&gt;Camera Branch：&lt;/strong&gt;         &lt;br&gt;
输出 predictions[&amp;ldquo;pose_enc&amp;rdquo;]，通过 pose_encoding_to_extri_intri 转换为 extrinsic（4×4 矩阵）和 intrinsic（3×3 矩阵）。         &lt;br&gt;
用于相机姿态估计和深度图到点云的转换。&lt;/p&gt;
&lt;h5 id=&#34;2-文件处理handle_uploads-函数&#34;&gt;2. 文件处理（handle_uploads 函数）
&lt;/h5&gt;&lt;p&gt;&lt;strong&gt;功能：&lt;/strong&gt;                      &lt;br&gt;
处理用户上传的视频或图像，创建临时目录（input_images_&lt;timestamp&gt;），存储图像文件。                &lt;br&gt;
视频会按每秒一帧提取为图像。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt; 1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 6
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 7
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 8
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 9
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;10
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;11
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;12
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;13
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;timestamp&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;datetime&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;now&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;strftime&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;%Y%m&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;%d&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;_%H%M%S_&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;%f&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;target_dir&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;sa&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;input_images_&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;timestamp&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;target_dir_images&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;os&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;path&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;join&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;target_dir&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;images&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;input_video&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;is&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;not&lt;/span&gt; &lt;span class=&#34;kc&#34;&gt;None&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;vs&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;cv2&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;VideoCapture&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;video_path&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;frame_interval&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;int&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;fps&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;  &lt;span class=&#34;c1&#34;&gt;# 1 frame/sec&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;k&#34;&gt;while&lt;/span&gt; &lt;span class=&#34;kc&#34;&gt;True&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;n&#34;&gt;gotit&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;frame&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;vs&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;read&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;not&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;gotit&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;            &lt;span class=&#34;k&#34;&gt;break&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;count&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;%&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;frame_interval&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;==&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;            &lt;span class=&#34;n&#34;&gt;image_path&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;os&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;path&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;join&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;target_dir_images&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;sa&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;video_frame_num&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;06&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;.png&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;            &lt;span class=&#34;n&#34;&gt;cv2&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;imwrite&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;image_path&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;frame&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;&lt;strong&gt;视频处理：&lt;/strong&gt; 使用 cv2.VideoCapture 按每秒一帧提取图像，保存为 PNG 文件。      &lt;br&gt;
&lt;strong&gt;图像处理：&lt;/strong&gt; 直接复制上传的图像到临时目录。         &lt;br&gt;
&lt;strong&gt;输出：&lt;/strong&gt; 临时目录路径（target_dir）和图像路径列表（image_paths）。         &lt;br&gt;
&lt;strong&gt;作用：&lt;/strong&gt; 为 run_model 提供输入图像目录，确保上传数据格式统一。&lt;/p&gt;
&lt;h5 id=&#34;3-界面更新update_gallery_on_upload-函数&#34;&gt;3. 界面更新（update_gallery_on_upload 函数）
&lt;/h5&gt;&lt;p&gt;&lt;strong&gt;功能：&lt;/strong&gt;             &lt;br&gt;
在用户上传视频或图像时，调用 handle_uploads 处理文件，并更新画廊显示上传的图像。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;4
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;not&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;input_video&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;and&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;not&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;input_images&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;kc&#34;&gt;None&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;kc&#34;&gt;None&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;kc&#34;&gt;None&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;kc&#34;&gt;None&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;target_dir&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;image_paths&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;handle_uploads&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;input_video&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;input_images&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;kc&#34;&gt;None&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;target_dir&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;image_paths&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;Upload complete. Click &amp;#39;Reconstruct&amp;#39; to begin 3D processing.&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;&lt;strong&gt;逻辑:&lt;/strong&gt; 检查是否有上传内容，若无则返回空值；否则处理文件并返回目录、图像路径和提示信息。             &lt;br&gt;
&lt;strong&gt;输出:&lt;/strong&gt; 更新 Gradio 画廊（image_gallery）和日志（log_output）。&lt;/p&gt;
&lt;h5 id=&#34;4-3d-重建gradio_demo-函数&#34;&gt;4. 3D 重建（gradio_demo 函数）
&lt;/h5&gt;&lt;p&gt;&lt;strong&gt;功能：&lt;/strong&gt;                        &lt;br&gt;
调用 run_model 进行推理，生成 3D 点云和相机参数。        &lt;br&gt;
使用 predictions_to_glb 将预测结果转换为 GLB 文件。            &lt;br&gt;
支持用户调整参数（如置信度阈值、帧过滤、天空分割）。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt; 1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 6
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 7
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 8
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 9
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;10
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;11
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;12
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;13
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;14
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;predictions&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;run_model&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;target_dir&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;model&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;np&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;savez&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;prediction_save_path&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;**&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;predictions&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;glbscene&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;predictions_to_glb&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;predictions&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;conf_thres&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;conf_thres&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;filter_by_frames&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;frame_filter&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;mask_black_bg&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;mask_black_bg&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;mask_white_bg&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;mask_white_bg&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;show_cam&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;show_cam&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;mask_sky&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;mask_sky&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;target_dir&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;target_dir&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;prediction_mode&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;prediction_mode&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;glbscene&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;export&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;file_obj&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;glbfile&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;&lt;strong&gt;推理：&lt;/strong&gt;  调用 run_model 获取预测。               &lt;br&gt;
&lt;strong&gt;保存：&lt;/strong&gt;  将预测结果保存为 .npz 文件（predictions.npz），便于后续可视化调整。               &lt;br&gt;
&lt;strong&gt;GLB 生成：&lt;/strong&gt;                 &lt;br&gt;
predictions_to_glb 根据 prediction_mode 选择使用 world_points（Pointmap Branch）或 world_points_from_depth（Depthmap + Camera Branch）。                            &lt;br&gt;
应用过滤条件（conf_thres、 mask_sky 等）优化点云。                       &lt;br&gt;
如果 show_cam=True，在 GLB 中显示相机位置。             &lt;br&gt;
&lt;strong&gt;输出：&lt;/strong&gt;  GLB 文件路径（glbfile）、日志信息和更新后的帧选择下拉菜单。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Pointmap Branch：&lt;/strong&gt; &lt;br&gt;
当 prediction_mode=&amp;ldquo;Pointmap Regression&amp;rdquo; 时，使用 predictions[&amp;ldquo;world_points&amp;rdquo;] 作为点云。     &lt;br&gt;
直接提供 3D 坐标，减少对相机参数的依赖。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Camera Branch：&lt;/strong&gt;&lt;br&gt;
提供 extrinsic 和 intrinsic，用于：           &lt;br&gt;
Depthmap 模式下生成 world_points_from_depth。             &lt;br&gt;
在 GLB 中可视化相机位置（show_cam=True）。&lt;/p&gt;
&lt;h5 id=&#34;5-辅助功能&#34;&gt;5. 辅助功能
&lt;/h5&gt;&lt;p&gt;&lt;strong&gt;clear_fields&lt;/strong&gt;：清除 3D 查看器、临时目录和画廊，恢复初始状态。              &lt;br&gt;
&lt;strong&gt;update_log&lt;/strong&gt;：显示“正在加载和重建”提示。             &lt;br&gt;
&lt;strong&gt;update_visualization&lt;/strong&gt;：               &lt;br&gt;
当用户调整参数（如 conf_thres、 frame_filter、 prediction_mode）时，加载保存的 .npz 文件，重新生成 GLB 文件。           &lt;br&gt;
避免重复运行推理，提高交互效率。               &lt;br&gt;
如果是示例数据（is_example=&amp;ldquo;True&amp;rdquo;），提示用户先点击“Reconstruct”。&lt;/p&gt;
&lt;h1 id=&#34;427&#34;&gt;4.27
&lt;/h1&gt;&lt;h2 id=&#34;nerf&#34;&gt;NeRF
&lt;/h2&gt;&lt;p&gt;bilbil &lt;a class=&#34;link&#34; href=&#34;https://www.bilibili.com/video/BV1CC411V7oq/?spm_id_from=333.337.search-card.all.click&amp;amp;vd_source=2f50930509c568406539e7a29e43c090&#34;  title=&#34;bilbil&#34;
     target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;【较真系列】讲人话-NeRF全解（原理+代码+公式）&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;CSDN &lt;a class=&#34;link&#34; href=&#34;https://blog.csdn.net/qq_45752541/article/details/130072505?ops_request_misc=%257B%2522request%255Fid%2522%253A%25226cb656f255cd45331db9dc80293f361e%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&amp;amp;request_id=6cb656f255cd45331db9dc80293f361e&amp;amp;biz_id=0&amp;amp;utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_positive~default-1-130072505-null-null.142%5ev102%5epc_search_result_base2&amp;amp;utm_term=nerf&amp;amp;spm=1018.2226.3001.4187&#34;  title=&#34;CSDN&#34;
     target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;【三维重建】NeRF原理+代码讲解&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;核心知识要点&#34;&gt;核心知识要点
&lt;/h2&gt;&lt;p&gt;Neural Radiance Fields (NeRF) 是一种通过神经网络从一组 2D 图像生成 3D 场景新视角的技术。它利用多层感知机 (MLP) 建模场景的辐射场，广泛用于 3D 重建和视图合成。以下是 NeRF 的核心知识要点，涵盖其表示、建模、渲染和训练过程。&lt;/p&gt;
&lt;h3 id=&#34;1-场景表示与位置编码&#34;&gt;1. 场景表示与位置编码
&lt;/h3&gt;&lt;p&gt;NeRF 将 3D 场景表示为一个连续的辐射场，通过 MLP 将输入的 3D 坐标 (x, y, z) 和视角方向 (θ, φ) 映射到颜色值 (r, g, b) 和体视密度 σ。体视密度描述了空间点的不透明度，颜色则依赖于位置和视角。&lt;/p&gt;
&lt;p&gt;为了捕捉高频细节，NeRF 对输入坐标和方向进行位置编码。这种编码通过正弦和余弦函数将低维输入映射到高维空间。坐标通常使用 10 个频率级别，视角方向使用 4 个频率级别，从而增强 MLP 对复杂纹理和几何的建模能力。&lt;/p&gt;
&lt;h3 id=&#34;2-体视渲染&#34;&gt;2. 体视渲染
&lt;/h3&gt;&lt;p&gt;NeRF 通过体视渲染生成 2D 图像。渲染过程沿相机光线采样多个点，计算每个点的密度和颜色，并通过积分生成像素颜色。透射率用于模拟光线穿过场景时的衰减，离散采样则将连续积分近似为加权和。这种方法能够精确模拟光线与场景的交互，生成逼真的视图。&lt;/p&gt;
&lt;h3 id=&#34;3-网络结构与训练&#34;&gt;3. 网络结构与训练
&lt;/h3&gt;&lt;p&gt;NeRF 的核心是一个 8 层全连接 MLP，每层 256 个神经元，使用 ReLU 激活函数。其工作流程如下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;输入经过位置编码的 3D 坐标，MLP 输出体视密度和中间特征向量。&lt;/li&gt;
&lt;li&gt;特征向量结合编码后的视角方向，输出最终的颜色值。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;训练时，NeRF 使用一组 2D 图像及其相机位姿作为输入。通过光线采样（每条光线采样多个点），计算渲染图像与真实图像的均方误差 (MSE) 作为损失函数。Adam 优化器用于更新 MLP 参数，逐步优化模型。&lt;/p&gt;
&lt;h3 id=&#34;4-分层采样优化&#34;&gt;4. 分层采样优化
&lt;/h3&gt;&lt;p&gt;为了提高渲染效率，NeRF 采用分层采样策略：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;粗网络&lt;/strong&gt;：在光线上均匀采样，预测初步的密度权重。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;细网络&lt;/strong&gt;：根据粗网络权重聚焦于高密度区域，优化采样点分布。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;粗网络和细网络的渲染结果均与真实图像比较，联合优化确保高质量的视图合成。这种分层方法有效减少计算量，同时保留细节。&lt;/p&gt;
&lt;h1 id=&#34;周总结&#34;&gt;周总结
&lt;/h1&gt;&lt;p&gt;效率较低&lt;br&gt;
大多时间浪费在调整学习方向上 &lt;br&gt;
下周教学评估 &lt;br&gt;
五一前尽量完成对3DGS大致的熟悉&lt;br&gt;
论文还是读的太少啦 加油加油 尽量在五月中旬前简历能凑够三个科研项目&lt;/p&gt;
</description>
        </item>
        <item>
        <title>前置知识</title>
        <link>https://Cyrus-hao.github.io/p/%E5%89%8D%E7%BD%AE%E7%9F%A5%E8%AF%86/</link>
        <pubDate>Wed, 23 Apr 2025 23:20:21 +0800</pubDate>
        
        <guid>https://Cyrus-hao.github.io/p/%E5%89%8D%E7%BD%AE%E7%9F%A5%E8%AF%86/</guid>
        <description>&lt;img src="https://Cyrus-hao.github.io/p/%E5%89%8D%E7%BD%AE%E7%9F%A5%E8%AF%86/cold_Crime.jpg" alt="Featured image of post 前置知识" /&gt;&lt;h2 id=&#34;部分学习路线&#34;&gt;部分学习路线
&lt;/h2&gt;&lt;p&gt;希望我的学习路线能对大家有所帮助 欢迎批评指正~&lt;/p&gt;
&lt;h3 id=&#34;249---251&#34;&gt;24.9 -&amp;gt; 25.1
&lt;/h3&gt;&lt;p&gt;江科大stm32(上手度高 与正点原子 野火等相比理论略显不足但有较高动手乐趣)   &lt;br&gt;
&lt;a class=&#34;link&#34; href=&#34;https://www.bilibili.com/video/BV1th411z7sn/?spm_id_from=333.337.search-card.all.click&#34;  title=&#34;STM32入门教程-2023版 细致讲解 中文字幕&#34;
     target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;bilibili&lt;/a&gt;  &lt;br&gt;
stm32循迹避障小车(当时买套件学纯代码 可手搓个麦轮小车比买套件有意思)  &lt;br&gt;
PCB绘图(嘉立创)  &lt;br&gt;
&lt;a class=&#34;link&#34; href=&#34;https://www.bilibili.com/video/BV1At421h7Ui/?spm_id_from=333.337.search-card.all.click&#34;  title=&#34;零基础入门PCB设计-国一学长带你学嘉立创EDA专业版 全程保姆级教学 中文字幕（大师篇已更新）&#34;
     target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;bilibili&lt;/a&gt;    &lt;br&gt;
正点原子电机控制(不推荐买课程电机驱动板 容易吃灰)   &lt;br&gt;
&lt;a class=&#34;link&#34; href=&#34;https://www.bilibili.com/video/BV1hv4y1g7s3/?spm_id_from=333.337.search-card.all.click&#34;  title=&#34;【正点原子】手把手教你学STM32电机应用控制&#34;
     target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;bilibili&lt;/a&gt;    &lt;br&gt;
matlab(感觉不用特意去学 数模快到前几天刷一遍就行) &lt;br&gt;
&lt;a class=&#34;link&#34; href=&#34;https://www.youtube.com/playlist?list=PLVHBjRDK0kALcQMwAFbR5q2driYZCHNIx&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;youtube&lt;/a&gt;&lt;br&gt;
24电赛省赛三子棋(视觉部分方法挺多 做着玩锻炼思维 做个完整项目进步还是较快的)  &lt;br&gt;
opencv(不是特别推荐我看的这个 都去看cs231吧haha)    &lt;br&gt;
&lt;a class=&#34;link&#34; href=&#34;https://www.bilibili.com/video/BV1PV411774y/?spm_id_from=333.337.search-card.all.click&amp;amp;vd_source=2f50930509c568406539e7a29e43c090&#34;  title=&#34;【B站最好的OpenCV课程推荐】OpenCV从入门到实战 全套课程&#34;
     target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;bilibili&lt;/a&gt;        &lt;br&gt;
cs231(部分 后期末考试 鸽)     &lt;br&gt;
&lt;a class=&#34;link&#34; href=&#34;https://www.youtube.com/watch?v=vT1JzLTH4G4&amp;amp;list=PL3FW7Lu3i5JvHM8ljYj-zLfQRF3EO8sYv&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;youtube&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;251---252&#34;&gt;25.1 -&amp;gt; 25.2
&lt;/h3&gt;&lt;p&gt;寒假直接忘本 化身纯摆小子hah 别学    &lt;br&gt;
线性代数(吴恩达老师把线代和机器学习混起来讲的 感觉不错 推)    &lt;br&gt;
&lt;a class=&#34;link&#34; href=&#34;https://www.bilibili.com/video/BV1Pg4y1X7Pa/?spm_id_from=333.337.search-card.all.click&#34;  title=&#34;吴恩达《机器学习数学基础（线性代数/微积分）》mathematics-for-machine-learning（中英字幕）&#34;
     target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;bilibili&lt;/a&gt;      &lt;br&gt;
西瓜书(仅仅止步于第三章 去图书馆边看边睡hah 大部分手撕数学原理 不推荐一开始直接去看)     &lt;br&gt;
蓝桥杯嵌入式(力推下面的up 比赛前看的 暑假看的那个模板有一定问题)   &lt;br&gt;
&lt;a class=&#34;link&#34; href=&#34;https://www.bilibili.com/video/BV1wi4y1172U/?spm_id_from=333.1007.top_right_bar_window_custom_collection.content.click&#34;  title=&#34;【备战2025蓝桥杯 嵌入式组】CT117E-M4 新款开发板 3小时省赛模块 速成总结&#34;
     target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;bilibili&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;252---254now&#34;&gt;25.2 -&amp;gt; 25.4(now)
&lt;/h3&gt;&lt;p&gt;机器学习(吴恩达 挺好就是后面字幕有一定问题 会干扰理解)  &lt;br&gt;
&lt;a class=&#34;link&#34; href=&#34;https://www.bilibili.com/video/BV1Bq421A74G/?spm_id_from=333.337.search-card.all.click&#34;  title=&#34;(超爽中英!) 2024公认最好的【吴恩达机器学习】教程！附课件代码 Machine Learning Specialization&#34;
     target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;bilibili&lt;/a&gt;              &lt;br&gt;
鱼书(神经网络部分直观 反向传播等理论通俗易懂 好！)  &lt;br&gt;
pytorch(小土堆 讲的超级棒 重视实践 我把他的视频都刷了遍hah)   &lt;br&gt;
&lt;a class=&#34;link&#34; href=&#34;https://www.bilibili.com/video/BV1hE411t7RN/?spm_id_from=333.1007.top_right_bar_window_custom_collection.content.click&#34;  title=&#34;PyTorch深度学习快速入门教程（绝对通俗易懂！）【小土堆】&#34;
     target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;bilibili&lt;/a&gt;         &lt;br&gt;
强化学习数学原理(公式部分有些看不懂 暂鸽)    &lt;br&gt;
&lt;a class=&#34;link&#34; href=&#34;https://www.bilibili.com/video/BV1sd4y167NS/?spm_id_from=333.337.search-card.all.click&#34;  title=&#34;【强化学习的数学原理】课程：从零开始到透彻理解（完结）&#34;
     target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;bilibili&lt;/a&gt;     &lt;br&gt;
动手学深度学习(在读ing 理论部分清晰 清晰记得当时信息论引入交叉熵豁然开朗)      &lt;br&gt;
李沐带读论文系列(在看ing 沐神还是挺有趣的哈哈) &lt;br&gt;
&lt;a class=&#34;link&#34; href=&#34;https://www.bilibili.com/video/BV1H44y1t75x/?spm_id_from=333.1387.homepage.video_card.click&amp;amp;vd_source=2f50930509c568406539e7a29e43c090&#34;  title=&#34;如何读论文&#34;
     target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;bilibili&lt;/a&gt;&lt;/p&gt;
</description>
        </item>
        <item>
        <title>Introduction</title>
        <link>https://Cyrus-hao.github.io/p/introduction/</link>
        <pubDate>Wed, 23 Apr 2025 22:46:08 +0800</pubDate>
        
        <guid>https://Cyrus-hao.github.io/p/introduction/</guid>
        <description>&lt;img src="https://Cyrus-hao.github.io/p/introduction/flower.jpg" alt="Featured image of post Introduction" /&gt;&lt;h2 id=&#34;写在十九岁的起点&#34;&gt;写在十九岁的起点
&lt;/h2&gt;&lt;p&gt;今天是我的19岁生日。没有往年的热闹，我选择静静地为自己留一片空间。它不必完美，或许有些凌乱，或许带着点稚气，但它真实，属于我。高考失利让我来到一所不太理想的大学，可这一学期多以来，遇到的好老师和朋友让我觉得，或许一切自有安排。资源有限，探索新东西时总觉得力不从心，抱怨过，但也只能接受。希望通过写博客，记录生活的点滴，反思自己，一步步走向更远的未来，找回属于我的光，慢慢定义人生。&lt;/p&gt;
</description>
        </item>
        <item>
        <title>归档</title>
        <link>https://Cyrus-hao.github.io/archives/</link>
        <pubDate>Tue, 28 May 2019 00:00:00 +0000</pubDate>
        
        <guid>https://Cyrus-hao.github.io/archives/</guid>
        <description></description>
        </item>
        <item>
        <title>关于</title>
        <link>https://Cyrus-hao.github.io/%E5%85%B3%E4%BA%8E/</link>
        <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
        
        <guid>https://Cyrus-hao.github.io/%E5%85%B3%E4%BA%8E/</guid>
        <description>&lt;p&gt;This is a test page for i18n support.&lt;/p&gt;
</description>
        </item>
        <item>
        <title>搜索</title>
        <link>https://Cyrus-hao.github.io/search/</link>
        <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
        
        <guid>https://Cyrus-hao.github.io/search/</guid>
        <description></description>
        </item>
        <item>
        <title>友情链接</title>
        <link>https://Cyrus-hao.github.io/%E5%8F%8B%E6%83%85%E9%93%BE%E6%8E%A5/</link>
        <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
        
        <guid>https://Cyrus-hao.github.io/%E5%8F%8B%E6%83%85%E9%93%BE%E6%8E%A5/</guid>
        <description>&lt;p&gt;To use this feature, add &lt;code&gt;links&lt;/code&gt; section to frontmatter.&lt;/p&gt;
&lt;p&gt;This page&amp;rsquo;s frontmatter:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;6
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;7
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;8
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;9
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nt&#34;&gt;links&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;- &lt;span class=&#34;nt&#34;&gt;title&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;GitHub&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;description&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;GitHub is the world&amp;#39;s largest software development platform.&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;website&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;https://github.com&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;image&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;https://github.githubassets.com/images/modules/logos_page/GitHub-Mark.png&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;- &lt;span class=&#34;nt&#34;&gt;title&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;TypeScript&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;description&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;TypeScript is a typed superset of JavaScript that compiles to plain JavaScript.&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;website&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;https://www.typescriptlang.org&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;image&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;ts-logo-128.jpg&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;&lt;code&gt;image&lt;/code&gt; field accepts both local and external images.&lt;/p&gt;
</description>
        </item>
        
    </channel>
</rss>
