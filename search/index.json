[{"content":"5.5 来自5.6的记录~ 昨日完成了深圳杯数模的实验 简历书写 以及game101前两课 作业未写 5.6 希望cad导能把鼠鼠收了吧🥹🥹 早上体育课老师不在 爽空出一节课时间\nlecture 3 2D: point (x,y,1) vector (x,y,0) 3D: point (x,y,z,1) vector (x,y,z,0) 矩阵*逆矩阵 = 单位矩阵 矩阵放向量左边 相当于左边的矩阵应用到右边的向量 矩阵变换从右到左 先进行线性变换再平移 2D transformations 5.7 games101目前进度在lecture6 状态有些低迷 满天柳絮啊 确实是对鼻炎患者的谋杀 今天初恋生日 哈哈已经托人祝福她了 英语周六周日再做吧 这两天打完数模 得多放些时间在nerf和3dgs一些前沿论文上了 论文还是读的太少了 games101 暂时先放放\nPlenoxels bilbil 【论文讲解】Plenoxels：没有神经的辐射场，把NeRF提速两个数量级(CVPR 2022)\n本文提出了Plenoxels，一个用于照片级视角合成的系统。Plenoxels将场景表达为一个稀疏的球谐函数(Spherical Harmonics)表示的体素网格 这种表达可以不使用任何神经网络，采用包含内外参的图像采用梯度方法和正则化进行优化。 NeRF的关键点并不是神经网络 而是可微体渲染 ","date":"2025-05-05T00:00:00Z","image":"https://Cyrus-hao.github.io/p/week11/1_hu_66c13d574bce2a9.png","permalink":"https://Cyrus-hao.github.io/p/week11/","title":"Week11"},{"content":"4.28 Denoising Diffusion Probabilistic Models 较真系列起手 这周真是有了吧\nbilbil 【较真系列】讲人话-Diffusion Model全解(原理+代码+公式)\nDDPM 核心知识要点 Denoising Diffusion Probabilistic Models (DDPM) 是一种基于概率生成模型的深度学习方法，通过逐步添加噪声和去噪的过程生成高质量样本，广泛应用于图像生成等领域。以下是 DDPM 的核心知识要点，涵盖其基本原理、模型结构和训练过程。\n1. 基本原理 DDPM 基于马尔可夫链，通过前向和后向过程实现数据生成：\n前向过程（扩散）：从真实数据 x_0 开始，逐步添加高斯噪声，经过 T 步生成近似高斯噪声 x_T。每一步的噪声由预定义的方差调度控制。 后向过程（去噪）：从噪声 x_T 开始，通过学习逆向分布逐步恢复原始数据 x_0。后向过程由神经网络参数化，预测去噪方向。 2. 模型结构 DDPM 通常采用 U-Net 作为去噪网络，输入为带噪声样本 x_t 和时间步 t，输出为预测的噪声或均值：\nU-Net 架构：包含下采样和上采样路径，结合残差连接和注意力机制，适合处理图像数据。 时间嵌入：将时间步 t 编码为正弦/余弦嵌入或全连接层，融入网络以区分不同去噪阶段。 输出目标通常为噪声 ε 或均值 μ，具体取决于优化目标。 3. 训练过程 DDPM 的训练目标是优化后向过程，使生成样本接近真实数据分布：\n损失函数：使用简化的变分下界，优化预测噪声与真实噪声的均方误差 (MSE)。流程为： 随机采样时间步 t，从真实数据 x_0 生成带噪样本 x_t。 网络预测噪声，计算预测与实际噪声的 MSE。 采样：训练时随机选择时间步 t 和数据样本，确保模型学习所有去噪阶段。 优化：使用 Adam 优化器通过梯度下降更新网络参数。 4. 推理与采样 生成样本时，从纯高斯噪声 x_T 开始，逐步执行后向去噪过程：\n逐层去噪：根据学习到的后向分布，从 x_T 到 x_0 迭代 T 步，每步采样生成下一状态。 调度优化：通过调整方差调度（如 cosine 调度）或加速方法（如 DDIM），减少采样步骤，提高效率。 生成样本质量高，但推理需要多次前向传播，速度较慢。 5. 关键特性 生成质量：DDPM 生成的样本保真度高，优于许多传统生成模型（如 GAN）。 训练稳定：基于概率框架，避免了 GAN 的对抗训练问题。 灵活性：支持条件生成（如文本到图像）和其他模态（如音频）。 4.29 不断告诉自己再不做简历项目就没时间了 真的很难改掉放假前不想学习的心态 明明还有一天 加油\n理论 bilbil 3D Gaussian Splatting原理速通 bilbil 从NeRF到3DGS 实践 bilbil 从NeRF到3DGS\n4.30 放假前最后一天 蓝桥杯最后也只是堪堪省三啊 鼠鼠再也不打嵌入式组了🥲 早上完善了yolo多自注意力+剪枝的项目 找到了更简单的3dgs实现全流程视频 bilbil 3D Gaussian Splatting本地部署 5.3 返校第一天哈 感觉没干什么事 心还在大连。。 深圳杯数模堪堪用ai写完了代码 晚上陪同学过生日聊了会 希望无用的内卷能少些吧。。 希望明日早起高效些 简历还缺1项目 这两天尽量在数模完成的同时能大部分构思出 下周结束前一定要完成简历le 加油\n5.4 假期倒数啦 找到篇较好的知乎贴DleX 跟着经验贴完成入门工作似乎也是优解\n1.速通3DGS原理 Instant-NGP的速度达到Mip-Nerf360的渲染质量\n1.1 sfm初始化稀疏点云 通过colmap创建初始化点云\n1.2 3D高斯椭球集的创建 位置信息：点云信息初始化(x,y,z) 高斯椭球中心点(均值) 形状信息：高斯椭球的协方差矩阵 Y方向压缩再绕旋转原点一定角度 颜色信息：点云颜色(r,g,b)\u0026ndash;用球谐函数来表示 使得点云在不同角度呈现不同颜色 不透明度信息：点云不透明度 球谐函数：与仰角和方位角有关 与半径无关\n1.3 计算机图形学投影矩阵 1.4 渲染公式 权重值*不透明度*颜色值\n1.5 loss定义 1.6 基于梯度自适应改变点云的分布方式 pruning离相机近的点和不透明度低于设置的阈值的点 过度重构和欠采样 方差很小 \u0026ndash; 克隆高斯来适应 方差很大 \u0026ndash; 分割成两个高斯\n2.【论文讲解】用点云结合3D高斯构建辐射场，成为快速训练、实时渲染的新SOTA！ bilbil 【论文讲解】用点云结合3D高斯构建辐射场，成为快速训练、实时渲染的新SOTA！ 总结：从初始sfm点云出发 以每个点为中心生成3D高斯 用相机参数把点投影到图像平面上(splatting) 从splatting的痕迹中进行tile-based的光栅化得到渲染图象 将渲染图像和 GT图像求loss反向传播 自适应的密度控制模块根据传递到点上的梯度来决定是否需要对3D高斯做分割或者克隆 梯度也会传递到3D高斯里来更新其中储存的位置 协方差矩阵 不透明度等 参数 核心:构建以协方差矩阵为主导的3D高斯点云 围绕3D高斯点云进行渲染和优化\n","date":"2025-04-26T00:00:00Z","image":"https://Cyrus-hao.github.io/p/week10/1_hu_74eff8456d041eac.png","permalink":"https://Cyrus-hao.github.io/p/week10/","title":"Week10"},{"content":"4.26 鼠鼠打算试试申暑研 善良的老师推了zju的一位做3DGS的大牛 遂开始换方向开始3DGS方向的学习 昨晚和做定位的高手学长聊了下 收获满满捏 周六较闲 开始该方向论文的检索\nVGGT: Visual Geometry Grounded Transformer 介绍了一种前馈神经网络VGGT，能够从单个或多个图像视图中直接预测场景的3D属性，包括相机参数、点图、深度图和3D点轨迹。该模型在多个3D任务中表现出色，如相机参数估计、多视图深度估计和密集点云重建，且处理速度快（不到一秒）。 (一) 输入图像处理 图像分块与特征提取: VGGT首先将输入图像分成小块，使用DINOv2(选择DINOv2是因为其在训练早期阶段表现更稳定，且对超参数（如学习率和动量）不敏感，相比之下，14×14的卷积层在初期可能不稳定)提取特征生成tokens，并添加位置嵌入以保留空间信息 这一步确保模型能稳定处理图像。\n位置嵌入: 在tokens上添加位置嵌入 以保留图像的空间关系。这一步对于保持transformer对空间信息的感知至关重要。\n(二) Transformer架构 交替注意力机制： 模型采用24层的transformer，使用交替注意力机制，交替处理帧内和全局信息。每个注意力层有1024维特征和16个头，类似于DINOv2的ViT-L配置。\n注意力机制交替在帧内（frame-wise）和全局（global）之间工作： 帧内注意力： 关注单个图像帧内的局部细节，适合捕捉细粒度的特征。\n全局注意力： 跨多个视图理解全局场景，适合处理多视图一致性。\n(三) 特征提取与上采样 从特定层(4、11、17和23)提取tokens，通过DPT进行上采样，生成如深度图的高分辨率输出。这一步支持3D属性的精确预测。\n(四) 训练数据增强 图像增强技术： 在训练过程中，随机应用以下增强，以提高模型对各种光照和噪声条件的鲁棒性：\n颜色抖动（Color Jittering）：调整图像的亮度、对比度、饱和度和色调，模拟不同光照条件。\n高斯模糊（Gaussian Blur）：应用模糊效果，模拟真实世界中的成像模糊。\n灰度增强（Grayscale Augmentation）：将图像转换为灰度，测试模型在无颜色信息时的性能。\n图像尺寸调整：\n图像、深度图和点图被等比例调整，使最大维度为518像素。\n较短的维度被裁剪到168~518像素之间，且必须是14像素的倍数，以匹配patchification的patch大小。这一步确保了输入数据的统一性和兼容性。\n(五) 总结 Pointmap Branch 和 Camera Branch Camera Branch Pointmap Branch Code 代码文件初步评估 demo_gradio.py 作用\n功能： 提供一个基于 Gradio 的交互式 Web 界面，允许用户上传图像或视频，进行 3D 点云重建，并通过 3D 查看器可视化结果（以 GLB 格式输出）。 核心任务： 处理用户上传的视频（提取帧）或图像，存储到临时目录。 使用 VGGT 模型进行推理，生成 3D 点云（Pointmap Branch 或 Depthmap + Camera Branch）和相机参数。 将预测结果转换为 GLB 文件，显示在 Gradio 的 3D 查看器中。 支持交互式参数调整（如置信度阈值、天空过滤、帧选择、相机显示）。 提供示例数据（预定义视频）展示重建效果。 关键函数： run_model：运行 VGGT 模型推理，生成点云和相机参数。 handle_uploads：处理上传的视频/图像，创建临时目录。 gradio_demo：执行重建，生成 GLB 文件。 update_visualization：根据用户调整的参数更新可视化。 输出： 3D 点云和相机姿态的 GLB 文件。 交互式界面，包含画廊、3D 查看器和参数控件。 **依赖：**依赖 visual_util.py 的 predictions_to_glb 函数来生成 GLB 文件。 重要性 这是用户直接交互的主要入口，提供最直观的 3D 重建体验。 集成了完整的处理流程（上传、推理、可视化、参数调整），是系统的核心前端。 适合展示和测试 VGGT 模型的功能，广泛适用于研究、演示和开发。 示例数据和交互式界面使其对非技术用户友好，具有高可用性。 demo_viser.py 作用\n功能： 使用 Viser（一个 Python 3D 可视化库）提供基于 Web 的 3D 点云和相机姿态可视化，支持实时交互。 核心任务： 从 VGGT 模型的预测结果（点云、相机参数、置信度等）生成 3D 点云和相机视锥（frustum）。 通过 Viser 创建 Web 服务器，展示点云和相机姿态。 支持动态调整置信度阈值、帧选择和相机显示。 可选择使用 Pointmap Branch（world_points）或 Depthmap + Camera Branch（world_points_from_depth）。 支持天空分割（mask_sky）以过滤天空区域的点。 关键函数： viser_wrapper：主函数，设置 Viser 服务器，渲染点云和相机视锥。 apply_sky_segmentation：应用天空分割，过滤天空点。 main：加载模型，处理图像，运行推理，启动 Viser 可视化。 输出： 通过 Web 界面（默认端口 8080）展示的 3D 点云和相机姿态。 支持交互式调整（置信度、帧选择、相机显示）。 依赖： 依赖 visual_util.py 的 segment_sky 和 download_file_from_url 函数处理天空分割。 依赖 VGGT 模型相关的工具函数（load_and_preprocess_images, pose_encoding_to_extri_intri, unproject_depth_map_to_point_map）。 重要性 提供了一种替代 Gradio 的可视化方式，适合需要更高交互性或本地 Web 服务器的场景。 支持与 Gradio 类似的功能（点云、相机姿态、天空分割），但更专注于 3D 渲染，可能在某些场景下（如研究或调试）提供更好的可视化体验。 依赖命令行参数运行，适合技术用户或批量处理，但对非技术用户不如 Gradio 友好。 如果项目主要使用 Gradio 界面，Viser 的作用可能次要。 vggt_to_colmap.py 作用\n功能： 将 VGGT 模型的预测结果（点云、相机参数）转换为 COLMAP 格式的文件（cameras.txt/bin, images.txt/bin, points3D.txt/bin），用于后续 SfM（Structure from Motion）或 MVS（Multi-View Stereo）处理。 核心任务： 加载 VGGT 模型，处理输入图像，生成 3D 点云和相机参数。 过滤点云（基于置信度、天空、背景颜色），生成 COLMAP 兼容的 3D 点和 2D-3D 对应关系。 将相机内外参、姿态和点云数据写入 COLMAP 格式（支持文本或二进制文件）。 支持天空分割和背景过滤（黑色/白色）。 关键函数： process_images：运行 VGGT 模型推理，生成预测。 filter_and_prepare_points：过滤点云，生成 COLMAP 格式的 3D 点和 2D 投影。 write_colmap_*：写入 COLMAP 格式文件（cameras, images, points3D）。 segment_sky：应用天空分割（与 visual_util.py 共享）。 输出： COLMAP 格式文件（cameras.txt/bin, images.txt/bin, points3D.txt/bin），可用于 COLMAP 或其他 SfM/MVS 工具。 依赖： 依赖 visual_util.py 的 segment_sky 和 download_file_from_url 函数。 依赖 VGGT 模型相关工具函数。 重要性 对于需要将 VGGT 输出集成到 COLMAP 工作流（例如进一步优化相机姿态或生成稠密点云）的用户来说至关重要。 提供了从深度学习预测到传统 SfM/MVS 工具的桥梁，扩展了 VGGT 的应用场景。 依赖命令行运行，适合技术用户或自动化流程，但不如 Gradio 界面直观。 如果项目不涉及 COLMAP 或 SfM，其重要性较低。 visual_util.py 作用\n功能： 提供通用的工具函数，支持 3D 点云和相机姿态的处理、可视化以及天空分割。 核心任务： 将 VGGT 预测转换为 GLB 格式的 3D 场景（predictions_to_glb），用于 Gradio 和其他可视化。 处理相机姿态、点云过滤（置信度、天空、背景颜色）和场景对齐。 提供天空分割功能（segment_sky, run_skyseg），支持过滤天空区域的点。 提供文件下载功能（download_file_from_url），用于获取天空分割模型。 关键函数： predictions_to_glb：将点云和相机参数转换为 GLB 场景，支持过滤和相机可视化。 integrate_camera_into_scene：将相机模型添加到 3D 场景。 segment_sky：使用 ONNX 模型进行天空分割。 download_file_from_url：处理 Hugging Face 模型文件的下载。 输出： GLB 格式的 3D 场景（用于 Gradio 或其他工具）。 天空分割掩码（用于点云过滤）。 依赖： 依赖外部库（如 trimesh, onnxruntime）。 重要性 是其他三个文件（demo_gradio.py, demo_viser.py, vggt_to_colmap.py）的核心依赖，提供关键的点云处理和可视化功能。 predictions_to_glb 是 Gradio 界面生成 3D 可视化的基础。 天空分割和文件下载功能被多个文件共享，增强了点云过滤的通用性。 作为一个工具模块，其代码被广泛复用，是系统不可或缺的部分。 代码文件详细解析 demo_gradio.py 1. 模型加载与推理（run_model 函数） 功能： 从指定目录加载图像，运行 VGGT 模型推理，生成 3D 点云和相机参数。 支持 Pointmap Branch（直接预测 3D 点）和 Depthmap + Camera Branch（通过深度图和相机参数生成点云）。\n1 2 3 4 5 6 7 images = load_and_preprocess_images(image_names).to(device) with torch.no_grad(): with torch.cuda.amp.autocast(dtype=dtype): predictions = model(images) extrinsic, intrinsic = pose_encoding_to_extri_intri(predictions[\u0026#34;pose_enc\u0026#34;], images.shape[-2:]) depth_map = predictions[\u0026#34;depth\u0026#34;] world_points = unproject_depth_map_to_point_map(depth_map, predictions[\u0026#34;extrinsic\u0026#34;], predictions[\u0026#34;intrinsic\u0026#34;]) 输入： 图像目录（target_dir/images）中的图像文件。 预处理： 通过 load_and_preprocess_images 将图像转换为张量（形状 [S, C, H, W]，S 为图像数量）。 推理： 使用 VGGT 模型生成预测，包括 depth（深度图）、world_points（Pointmap Branch 的 3D 点图）、pose_enc（Camera Branch 的姿态编码）。 自动混合精度（torch.cuda.amp.autocast）提高推理效率。 后处理： pose_enc 转换为相机内外参（extrinsic 和 intrinsic）。 深度图通过 unproject_depth_map_to_point_map 结合相机参数生成 3D 点（world_points_from_depth）。 输出： predictions 字典，包含 depth、 world_points、 world_points_from_depth、 extrinsic、 intrinsic 等。\nPointmap Branch： 输出 predictions[\u0026ldquo;world_points\u0026rdquo;]，直接预测每个像素的 3D 坐标（形状 [S, H, W, 3]）。 在 prediction_mode=\u0026ldquo;Pointmap Regression\u0026rdquo; 时使用。 Camera Branch： 输出 predictions[\u0026ldquo;pose_enc\u0026rdquo;]，通过 pose_encoding_to_extri_intri 转换为 extrinsic（4×4 矩阵）和 intrinsic（3×3 矩阵）。 用于相机姿态估计和深度图到点云的转换。\n2. 文件处理（handle_uploads 函数） 功能： 处理用户上传的视频或图像，创建临时目录（input_images_），存储图像文件。 视频会按每秒一帧提取为图像。\n1 2 3 4 5 6 7 8 9 10 11 12 13 timestamp = datetime.now().strftime(\u0026#34;%Y%m%d_%H%M%S_%f\u0026#34;) target_dir = f\u0026#34;input_images_{timestamp}\u0026#34; target_dir_images = os.path.join(target_dir, \u0026#34;images\u0026#34;) if input_video is not None: vs = cv2.VideoCapture(video_path) frame_interval = int(fps * 1) # 1 frame/sec while True: gotit, frame = vs.read() if not gotit: break if count % frame_interval == 0: image_path = os.path.join(target_dir_images, f\u0026#34;{video_frame_num:06}.png\u0026#34;) cv2.imwrite(image_path, frame) 视频处理： 使用 cv2.VideoCapture 按每秒一帧提取图像，保存为 PNG 文件。 图像处理： 直接复制上传的图像到临时目录。 输出： 临时目录路径（target_dir）和图像路径列表（image_paths）。 作用： 为 run_model 提供输入图像目录，确保上传数据格式统一。\n3. 界面更新（update_gallery_on_upload 函数） 功能： 在用户上传视频或图像时，调用 handle_uploads 处理文件，并更新画廊显示上传的图像。\n1 2 3 4 if not input_video and not input_images: return None, None, None, None target_dir, image_paths = handle_uploads(input_video, input_images) return None, target_dir, image_paths, \u0026#34;Upload complete. Click \u0026#39;Reconstruct\u0026#39; to begin 3D processing.\u0026#34; 逻辑: 检查是否有上传内容，若无则返回空值；否则处理文件并返回目录、图像路径和提示信息。 输出: 更新 Gradio 画廊（image_gallery）和日志（log_output）。\n4. 3D 重建（gradio_demo 函数） 功能： 调用 run_model 进行推理，生成 3D 点云和相机参数。 使用 predictions_to_glb 将预测结果转换为 GLB 文件。 支持用户调整参数（如置信度阈值、帧过滤、天空分割）。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 predictions = run_model(target_dir, model) np.savez(prediction_save_path, **predictions) glbscene = predictions_to_glb( predictions, conf_thres=conf_thres, filter_by_frames=frame_filter, mask_black_bg=mask_black_bg, mask_white_bg=mask_white_bg, show_cam=show_cam, mask_sky=mask_sky, target_dir=target_dir, prediction_mode=prediction_mode, ) glbscene.export(file_obj=glbfile) 推理： 调用 run_model 获取预测。 保存： 将预测结果保存为 .npz 文件（predictions.npz），便于后续可视化调整。 GLB 生成： predictions_to_glb 根据 prediction_mode 选择使用 world_points（Pointmap Branch）或 world_points_from_depth（Depthmap + Camera Branch）。 应用过滤条件（conf_thres、 mask_sky 等）优化点云。 如果 show_cam=True，在 GLB 中显示相机位置。 输出： GLB 文件路径（glbfile）、日志信息和更新后的帧选择下拉菜单。\nPointmap Branch： 当 prediction_mode=\u0026ldquo;Pointmap Regression\u0026rdquo; 时，使用 predictions[\u0026ldquo;world_points\u0026rdquo;] 作为点云。 直接提供 3D 坐标，减少对相机参数的依赖。\nCamera Branch：\n提供 extrinsic 和 intrinsic，用于： Depthmap 模式下生成 world_points_from_depth。 在 GLB 中可视化相机位置（show_cam=True）。\n5. 辅助功能 clear_fields：清除 3D 查看器、临时目录和画廊，恢复初始状态。 update_log：显示“正在加载和重建”提示。 update_visualization： 当用户调整参数（如 conf_thres、 frame_filter、 prediction_mode）时，加载保存的 .npz 文件，重新生成 GLB 文件。 避免重复运行推理，提高交互效率。 如果是示例数据（is_example=\u0026ldquo;True\u0026rdquo;），提示用户先点击“Reconstruct”。\n4.27 NeRF bilbil 【较真系列】讲人话-NeRF全解（原理+代码+公式）\nCSDN 【三维重建】NeRF原理+代码讲解\n核心知识要点 Neural Radiance Fields (NeRF) 是一种通过神经网络从一组 2D 图像生成 3D 场景新视角的技术。它利用多层感知机 (MLP) 建模场景的辐射场，广泛用于 3D 重建和视图合成。以下是 NeRF 的核心知识要点，涵盖其表示、建模、渲染和训练过程。\n1. 场景表示与位置编码 NeRF 将 3D 场景表示为一个连续的辐射场，通过 MLP 将输入的 3D 坐标 (x, y, z) 和视角方向 (θ, φ) 映射到颜色值 (r, g, b) 和体视密度 σ。体视密度描述了空间点的不透明度，颜色则依赖于位置和视角。\n为了捕捉高频细节，NeRF 对输入坐标和方向进行位置编码。这种编码通过正弦和余弦函数将低维输入映射到高维空间。坐标通常使用 10 个频率级别，视角方向使用 4 个频率级别，从而增强 MLP 对复杂纹理和几何的建模能力。\n2. 体视渲染 NeRF 通过体视渲染生成 2D 图像。渲染过程沿相机光线采样多个点，计算每个点的密度和颜色，并通过积分生成像素颜色。透射率用于模拟光线穿过场景时的衰减，离散采样则将连续积分近似为加权和。这种方法能够精确模拟光线与场景的交互，生成逼真的视图。\n3. 网络结构与训练 NeRF 的核心是一个 8 层全连接 MLP，每层 256 个神经元，使用 ReLU 激活函数。其工作流程如下：\n输入经过位置编码的 3D 坐标，MLP 输出体视密度和中间特征向量。 特征向量结合编码后的视角方向，输出最终的颜色值。 训练时，NeRF 使用一组 2D 图像及其相机位姿作为输入。通过光线采样（每条光线采样多个点），计算渲染图像与真实图像的均方误差 (MSE) 作为损失函数。Adam 优化器用于更新 MLP 参数，逐步优化模型。\n4. 分层采样优化 为了提高渲染效率，NeRF 采用分层采样策略：\n粗网络：在光线上均匀采样，预测初步的密度权重。 细网络：根据粗网络权重聚焦于高密度区域，优化采样点分布。 粗网络和细网络的渲染结果均与真实图像比较，联合优化确保高质量的视图合成。这种分层方法有效减少计算量，同时保留细节。\n周总结 效率较低\n大多时间浪费在调整学习方向上 下周教学评估 五一前尽量完成对3DGS大致的熟悉\n论文还是读的太少啦 加油加油 尽量在五月中旬前简历能凑够三个科研项目\n","date":"2025-04-26T00:00:00Z","image":"https://Cyrus-hao.github.io/p/week9/p8_hu_a7a83eb15b306951.png","permalink":"https://Cyrus-hao.github.io/p/week9/","title":"Week9"},{"content":"部分学习路线 希望我的学习路线能对大家有所帮助 欢迎批评指正~\n24.9 -\u0026gt; 25.1 江科大stm32(上手度高 与正点原子 野火等相比理论略显不足但有较高动手乐趣) bilibili stm32循迹避障小车(当时买套件学纯代码 可手搓个麦轮小车比买套件有意思) PCB绘图(嘉立创) bilibili 正点原子电机控制(不推荐买课程电机驱动板 容易吃灰) bilibili matlab(感觉不用特意去学 数模快到前几天刷一遍就行) youtube\n24电赛省赛三子棋(视觉部分方法挺多 做着玩锻炼思维 做个完整项目进步还是较快的) opencv(不是特别推荐我看的这个 都去看cs231吧haha) bilibili cs231(部分 后期末考试 鸽) youtube\n25.1 -\u0026gt; 25.2 寒假直接忘本 化身纯摆小子hah 别学 线性代数(吴恩达老师把线代和机器学习混起来讲的 感觉不错 推) bilibili 西瓜书(仅仅止步于第三章 去图书馆边看边睡hah 大部分手撕数学原理 不推荐一开始直接去看) 蓝桥杯嵌入式(力推下面的up 比赛前看的 暑假看的那个模板有一定问题) bilibili\n25.2 -\u0026gt; 25.4(now) 机器学习(吴恩达 挺好就是后面字幕有一定问题 会干扰理解) bilibili 鱼书(神经网络部分直观 反向传播等理论通俗易懂 好！) pytorch(小土堆 讲的超级棒 重视实践 我把他的视频都刷了遍hah) bilibili 强化学习数学原理(公式部分有些看不懂 暂鸽) bilibili 动手学深度学习(在读ing 理论部分清晰 清晰记得当时信息论引入交叉熵豁然开朗) 李沐带读论文系列(在看ing 沐神还是挺有趣的哈哈) bilibili\n","date":"2025-04-23T23:20:21+08:00","image":"https://Cyrus-hao.github.io/p/%E5%89%8D%E7%BD%AE%E7%9F%A5%E8%AF%86/cold_Crime_hu_2bdc603547b3c554.jpg","permalink":"https://Cyrus-hao.github.io/p/%E5%89%8D%E7%BD%AE%E7%9F%A5%E8%AF%86/","title":"前置知识"},{"content":"写在十九岁的起点 今天是我的19岁生日。没有往年的热闹，我选择静静地为自己留一片空间。它不必完美，或许有些凌乱，或许带着点稚气，但它真实，属于我。高考失利让我来到一所不太理想的大学，可这一学期多以来，遇到的好老师和朋友让我觉得，或许一切自有安排。资源有限，探索新东西时总觉得力不从心，抱怨过，但也只能接受。希望通过写博客，记录生活的点滴，反思自己，一步步走向更远的未来，找回属于我的光，慢慢定义人生。\n","date":"2025-04-23T22:46:08+08:00","image":"https://Cyrus-hao.github.io/p/introduction/flower_hu_159fccfeb8a0a588.jpg","permalink":"https://Cyrus-hao.github.io/p/introduction/","title":"Introduction"}]